{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from implementations import *\n",
    "from proj1_helpers import create_csv_submission, load_csv_data, predict_labels_kaggle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = -1\n",
    "UPPER_BOUND = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"competition-data/\"\n",
    "DATA_TEST = \"test.csv\"\n",
    "DATA_TRAIN = \"train.csv\"\n",
    "y_train, x_train, ids_train = load_csv_data(DATA_FOLDER + DATA_TRAIN, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train = x_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nfor i in range(29):\\n    for j in range(i+1, 30):\\n        fig = plt.figure()\\n        ax = fig.add_subplot(1, 1, 1)\\n        x = tx_train[i]\\n        y = tx_train[j]\\n        plt.xlabel('Feature %d' % i)\\n        plt.ylabel('Feature %d' % j)\\n        ax.scatter(x, y)\\n        plt.show()\\n\""
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to plot correlations between features\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i in range(29):\n",
    "    for j in range(i+1, 30):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        x = tx_train[i]\n",
    "        y = tx_train[j]\n",
    "        plt.xlabel('Feature %d' % i)\n",
    "        plt.ylabel('Feature %d' % j)\n",
    "        ax.scatter(x, y)\n",
    "        plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into 4 sets depending on the feature 22: 'PRI_jet_num' (categorical feature)\n",
    "masks_jet_train = get_jet_masks(tx_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntx_train, y_train = balance(tx_train.T, y_train, LOWER_BOUND, UPPER_BOUND)\\ntx_train.shape\\n'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the data set to have the same number of 1 that -1 (downsample)\n",
    "'''\n",
    "tx_train, y_train = balance(tx_train.T, y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace remaining NaN values with the median of the column + standardie the values of all features\n",
    "tx_train = replace_nan_by_median(tx_train)\n",
    "mean_train, std_train, tx_train = standardize(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST SET ###\n",
    "\n",
    "# Do the same we did to the training set, but to the test set\n",
    "y_test, x_test, ids_test = load_csv_data(DATA_FOLDER + DATA_TEST, LOWER_BOUND, UPPER_BOUND)\n",
    "masks_jet_test = get_jet_masks(x_test)\n",
    "tx_test = x_test.T\n",
    "tx_test = replace_nan_by_median(tx_test)\n",
    "tx_test = standardize_predef(tx_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 99913)\n",
      "(99913, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cross validate the different models one by one (change manually)\n",
    "model_nb = 0\n",
    "\n",
    "current_tx_train = tx_train.T[masks_jet_train[model_nb]].T\n",
    "current_y_train = y_train[masks_jet_train[model_nb]]\n",
    "\n",
    "# Those should be done if we choose not to do them before separating the data set\n",
    "'''\n",
    "# Remove columns full of NaN\n",
    "current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "\n",
    "# Remove columns without standard deviation at all\n",
    "current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "\n",
    "# Balance the data set\n",
    "current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "# Replace remaining NaN by median\n",
    "current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "\n",
    "# Standardize features\n",
    "mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "'''\n",
    "\n",
    "print(current_tx_train.shape)\n",
    "print(current_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: (5, 50, 64, 0, 1.0000000000000001e-05, 0.83649284355920328)\n",
      "Finished: (5, 50, 64, 0, 2.1544346900318823e-05, 0.8365428885997398)\n",
      "Finished: (5, 50, 64, 0, 4.6415888336127818e-05, 0.83653287959163247)\n",
      "Finished: (5, 50, 64, 0, 0.0001, 0.83629266339705732)\n",
      "Finished: (5, 50, 64, 0, 0.00021544346900318823, 0.83629266339705732)\n",
      "Finished: (5, 50, 64, 0, 0.00046415888336127773, 0.83615253728355532)\n",
      "Finished: (5, 50, 64, 0, 0.001, 0.83615253728355532)\n",
      "Finished: (5, 50, 64, 0, 0.0021544346900318821, 0.83578220398358527)\n",
      "Finished: (5, 50, 64, 0, 0.0046415888336127772, 0.83521169052146915)\n",
      "Finished: (5, 50, 64, 0, 0.01, 0.83472124912421175)\n",
      "Finished: (6, 50, 64, 0, 1.0000000000000001e-05, 0.83623260934841359)\n",
      "Finished: (6, 50, 64, 0, 2.1544346900318823e-05, 0.83766389750775683)\n",
      "Finished: (6, 50, 64, 0, 4.6415888336127818e-05, 0.80448403563206894)\n",
      "Finished: (6, 50, 64, 0, 0.0001, 0.83708337503753383)\n",
      "Finished: (6, 50, 64, 0, 0.00021544346900318823, 0.8031528375537984)\n",
      "Finished: (6, 50, 64, 0, 0.00046415888336127773, 0.83738364528075271)\n",
      "Finished: (6, 50, 64, 0, 0.001, 0.82448203383044749)\n",
      "Finished: (6, 50, 64, 0, 0.0021544346900318821, 0.82551296166549881)\n",
      "Finished: (6, 50, 64, 0, 0.0046415888336127772, 0.8364628165348813)\n",
      "Finished: (6, 50, 64, 0, 0.01, 0.80360324291862673)\n",
      "Finished: (7, 50, 64, 0, 1.0000000000000001e-05, 0.83767390651586437)\n",
      "Finished: (7, 50, 64, 0, 2.1544346900318823e-05, 0.83815433890501456)\n",
      "Finished: (7, 50, 64, 0, 4.6415888336127818e-05, 0.83854469022119904)\n",
      "Finished: (7, 50, 64, 0, 0.0001, 0.83574216795115608)\n",
      "Finished: (7, 50, 64, 0, 0.00021544346900318823, 0.83395055549995001)\n",
      "Finished: (7, 50, 64, 0, 0.00046415888336127773, 0.81699529576618968)\n",
      "Finished: (7, 50, 64, 0, 0.001, 0.83961565408867977)\n",
      "Finished: (7, 50, 64, 0, 0.0021544346900318821, 0.83939545591031917)\n",
      "Finished: (7, 50, 64, 0, 0.0046415888336127772, 0.83786407766990278)\n",
      "Finished: (7, 50, 64, 0, 0.01, 0.83792413171854674)\n",
      "Finished: (8, 50, 64, 0, 1.0000000000000001e-05, 0.84182764488039241)\n",
      "Finished: (8, 50, 64, 0, 2.1544346900318823e-05, 0.84223801421279154)\n",
      "Finished: (8, 50, 64, 0, 4.6415888336127818e-05, 0.84096687018316485)\n",
      "Finished: (8, 50, 64, 0, 0.0001, 0.84206786107496756)\n",
      "Finished: (8, 50, 64, 0, 0.00021544346900318823, 0.84204784305875291)\n",
      "Finished: (8, 50, 64, 0, 0.00046415888336127773, 0.84171754579121194)\n",
      "Finished: (8, 50, 64, 0, 0.001, 0.83123811430287264)\n",
      "Finished: (8, 50, 64, 0, 0.0021544346900318821, 0.84056650985887293)\n",
      "Finished: (8, 50, 64, 0, 0.0046415888336127772, 0.84070663597237516)\n",
      "Finished: (8, 50, 64, 0, 0.01, 0.83951556400760696)\n",
      "Finished: (9, 50, 64, 0, 1.0000000000000001e-05, 0.84170753678310484)\n",
      "Finished: (9, 50, 64, 0, 2.1544346900318823e-05, 0.84213792413171862)\n",
      "Finished: (9, 50, 64, 0, 4.6415888336127818e-05, 0.84248823941547391)\n",
      "Finished: (9, 50, 64, 0, 0.0001, 0.84225803222900608)\n",
      "Finished: (9, 50, 64, 0, 0.00021544346900318823, 0.84193774396957277)\n",
      "Finished: (9, 50, 64, 0, 0.00046415888336127773, 0.84171754579121205)\n",
      "Finished: (9, 50, 64, 0, 0.001, 0.84132719447502746)\n",
      "Finished: (9, 50, 64, 0, 0.0021544346900318821, 0.84156741066960272)\n",
      "Finished: (9, 50, 64, 0, 0.0046415888336127772, 0.84114703232909616)\n",
      "Finished: (9, 50, 64, 0, 0.01, 0.84035632068861976)\n",
      "Finished: (10, 50, 64, 0, 1.0000000000000001e-05, 0.8373836452807526)\n",
      "Finished: (10, 50, 64, 0, 2.1544346900318823e-05, 0.84050645581022909)\n",
      "Finished: (10, 50, 64, 0, 4.6415888336127818e-05, 0.83896506856170561)\n",
      "Finished: (10, 50, 64, 0, 0.0001, 0.80611550395355813)\n",
      "Finished: (10, 50, 64, 0, 0.00021544346900318823, 0.84222800520468422)\n",
      "Finished: (10, 50, 64, 0, 0.00046415888336127773, 0.83970573516164537)\n",
      "Finished: (10, 50, 64, 0, 0.001, 0.84220798718846956)\n",
      "Finished: (10, 50, 64, 0, 0.0021544346900318821, 0.84122710439395454)\n",
      "Finished: (10, 50, 64, 0, 0.0046415888336127772, 0.84103693323991602)\n",
      "Finished: (10, 50, 64, 0, 0.01, 0.83829446501851668)\n",
      "Finished: (11, 50, 64, 0, 1.0000000000000001e-05, 0.81456310679611632)\n",
      "Finished: (11, 50, 64, 0, 2.1544346900318823e-05, 0.80563507156440795)\n",
      "Finished: (11, 50, 64, 0, 4.6415888336127818e-05, 0.83755379841857669)\n",
      "Finished: (11, 50, 64, 0, 0.0001, 0.83847462716444787)\n",
      "Finished: (11, 50, 64, 0, 0.00021544346900318823, 0.83838454609148239)\n",
      "Finished: (11, 50, 64, 0, 0.00046415888336127773, 0.84140726653988585)\n",
      "Finished: (11, 50, 64, 0, 0.001, 0.84214793313982583)\n",
      "Finished: (11, 50, 64, 0, 0.0021544346900318821, 0.84221799619657689)\n",
      "Finished: (11, 50, 64, 0, 0.0046415888336127772, 0.841347212491242)\n",
      "Finished: (11, 50, 64, 0, 0.01, 0.83966569912921629)\n",
      "Finished: (12, 50, 64, 0, 1.0000000000000001e-05, 0.67856070463417084)\n",
      "Finished: (12, 50, 64, 0, 2.1544346900318823e-05, 0.71688519667700934)\n",
      "Finished: (12, 50, 64, 0, 4.6415888336127818e-05, 0.73629266339705746)\n",
      "Finished: (12, 50, 64, 0, 0.0001, 0.78297467720948855)\n",
      "Finished: (12, 50, 64, 0, 0.00021544346900318823, 0.78072265038534683)\n",
      "Finished: (12, 50, 64, 0, 0.00046415888336127773, 0.76432789510559496)\n",
      "Finished: (12, 50, 64, 0, 0.001, 0.76961265138624779)\n",
      "Finished: (12, 50, 64, 0, 0.0021544346900318821, 0.78315483935541985)\n",
      "Finished: (12, 50, 64, 0, 0.0046415888336127772, 0.77769992993694315)\n",
      "Finished: (12, 50, 64, 0, 0.01, 0.82738464618156338)\n",
      "Finished: (13, 50, 64, 0, 1.0000000000000001e-05, 0.61102992693424085)\n",
      "Finished: (13, 50, 64, 0, 2.1544346900318823e-05, 0.65034531077970181)\n",
      "Finished: (13, 50, 64, 0, 4.6415888336127818e-05, 0.65755179661695518)\n",
      "Finished: (13, 50, 64, 0, 0.0001, 0.68639775798218383)\n",
      "Finished: (13, 50, 64, 0, 0.00021544346900318823, 0.69175257731958761)\n",
      "Finished: (13, 50, 64, 0, 0.00046415888336127773, 0.70711640476428783)\n",
      "Finished: (13, 50, 64, 0, 0.001, 0.72978680812731456)\n",
      "Finished: (13, 50, 64, 0, 0.0021544346900318821, 0.73402061855670098)\n",
      "Finished: (13, 50, 64, 0, 0.0046415888336127772, 0.78284456010409365)\n",
      "Finished: (13, 50, 64, 0, 0.01, 0.72081873686317688)\n"
     ]
    }
   ],
   "source": [
    "from implementations import build_k_indices\n",
    "#from tqdm import tqdm_notebook\n",
    "from matplotlib.pyplot import figure, show\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def cross_validation_demo():\n",
    "    model = \"ridge_regression\"\n",
    "    seed = 3\n",
    "    k_fold = 10\n",
    "    degrees = np.arange(5, 14, 1)\n",
    "    lambdas = np.logspace(-5, -2, 10)\n",
    "    gammas = [0]#np.arange(0.05, 0.6, 0.05)\n",
    "    #initial_w = init_w(tx_train)\n",
    "    max_iters = [50]#np.logspace(2, 3, 4)\n",
    "    batch_sizes = [64]\n",
    "    k_indices = build_k_indices(current_y_train, k_fold, seed)\n",
    "    results = []\n",
    "    for degree in degrees:\n",
    "        tx_train_poly = build_poly_tx(current_tx_train, degree)\n",
    "        initial_w = init_w(tx_train_poly)\n",
    "        for max_iter in max_iters:\n",
    "            for batch_size in batch_sizes:\n",
    "                for gamma in gammas:\n",
    "                    for lambda_ in lambdas:\n",
    "                        accs = []\n",
    "                        ws = []\n",
    "                        for k in range(k_fold):\n",
    "                            w_tr, acc = cross_validation(current_y_train, tx_train_poly.T, initial_w,\n",
    "                                                         int(max_iter), k_indices, k, gamma, lambda_, LOWER_BOUND, UPPER_BOUND, model, batch_size)\n",
    "                            ws.append(w_tr)\n",
    "                            accs.append(acc)\n",
    "                        w_mean = np.mean(ws, axis=0)\n",
    "                        acc_mean = np.mean(accs)\n",
    "                        results.append((degree, max_iter, batch_size, gamma, lambda_, acc_mean, w_mean))\n",
    "\n",
    "                        print(\"Finished: \" + str((degree, max_iter, batch_size, gamma, lambda_, acc_mean)))\n",
    "\n",
    "                        '''\n",
    "                        fig = plt.subplots(1, 1, figsize=(10,5))\n",
    "                        plt.plot(range(1,k_fold+1), accs, marker=\".\", color='b', label='accuracy')\n",
    "                        plt.axhline(y=acc_mean, color='r', label='mean')\n",
    "                        ax = plt.gca()\n",
    "                        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                        plt.xlabel(\"k\")\n",
    "                        plt.ylabel(\"accuracy\")\n",
    "                        plt.title(\"k-fold accuracy for lambda=%.5f, gamma=%.2f, degree=%d\" % (lambda_, gamma, degree))\n",
    "                        plt.legend(loc=2)\n",
    "                        plt.grid(True)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 9, 8, 10]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort(key=lambda x: -x[5])\n",
    "[x[0] for x in results[:4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating into multiple models\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 0, 0, 0, 0.00046415888336127773, 0.81200248756218907)\n",
    "* For 1: Best is Ridge (11, 0, 0, 0, 0.001291549665014884, 0.79104143337066068)\n",
    "* For 2: Best is Ridge (11, 50, 64, 0, 4.6415888336127818e-05, 0.84399274987053341)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 1.0000000000000001e-05, 0.80668918918918919)\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 0.001, 0.81080816006276968)\n",
    "* For 1: Best is Ridge (13, 50, 64, 0, 0.001, 0.79271021291952359)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83351592615134906)\n",
    "* For 3: Best is Ridge (10, 50, 64, 0, 0.001, 0.80564635958395248)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 50, 64, 0, 4.6415888336127818e-05, 0.84248823941547391)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.001, 0.80530049006964144)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83388921977367492)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 2.1544346900318823e-05, 0.82996389891696754)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 2.1544346900318823e-05, 0.84242818536683006)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.80531338663915408)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.83325392098471307)\n",
    "* For 3: Best is Ridge (12, 50, 64, 0, 0.0001, 0.8336642599277978)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.842863291063\n",
      "Accuracy: 0.806909625503\n",
      "Accuracy: 0.836658131364\n",
      "Accuracy: 0.835814834867\n",
      "Number of 1: 177264\n",
      "Number of -1: 390974\n"
     ]
    }
   ],
   "source": [
    "# Code to use all models on the test_set and have the final prediction in y_pred\n",
    "\n",
    "degrees = [9, 12, 12, 11]\n",
    "lambdas = [4.6415888336127818e-05, 0.001, 0.0021544346900318821, 2.1544346900318823e-05]\n",
    "\n",
    "# Final prediction in here\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "for i in range(len(masks_jet_train)):\n",
    "    current_tx_train = tx_train.T[masks_jet_train[i]].T\n",
    "    current_tx_test = tx_test.T[masks_jet_test[i]].T\n",
    "    current_y_train = y_train[masks_jet_train[i]]\n",
    "    \n",
    "    # Preprocess here if not preprocessed before separating\n",
    "    '''\n",
    "    # Remove columns full of NaN\n",
    "    current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "    current_tx_test = current_tx_test[~np.all(np.isnan(current_tx_test), axis=1)]\n",
    "\n",
    "    # Remove columns without standard deviation at all\n",
    "    current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "    current_tx_test = current_tx_test[np.nanstd(current_tx_test, axis=1) != 0]\n",
    "    \n",
    "    # Balance the data set\n",
    "    current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "    # Replace remaining NaN by median\n",
    "    current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "    current_tx_test = replace_nan_by_median(current_tx_test)\n",
    "\n",
    "    # Standardize features\n",
    "    mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "    current_tx_test = standardize_predef(current_tx_test, mean_train, std_train)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Build poly\n",
    "    current_tx_poly_train = build_poly_tx(current_tx_train, degrees[i])\n",
    "    current_tx_poly_test = build_poly_tx(current_tx_test, degrees[i])\n",
    "    \n",
    "    # Compute best method\n",
    "    current_w, current_loss = ridge_regression(current_y_train, current_tx_poly_train, lambdas[i])\n",
    "    \n",
    "    acc = accuracy(current_y_train, current_tx_poly_train.T, current_w, LOWER_BOUND, UPPER_BOUND)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    \n",
    "    # Predict\n",
    "    y_test_pred = predict_labels_kaggle(current_w, current_tx_poly_test.T, LOWER_BOUND, UPPER_BOUND)\n",
    "    y_pred[masks_jet_test[i]] = y_test_pred.flatten()\n",
    "\n",
    "print(\"Number of %d:\" % UPPER_BOUND, np.count_nonzero(y_pred == UPPER_BOUND))\n",
    "print(\"Number of %d:\" % LOWER_BOUND, np.count_nonzero(y_pred == LOWER_BOUND))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"test20.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test n : algorithm / features / y / w\n",
    "- - - - - - - - - - - - - - - - - - - \n",
    "Test 1 : least_squares / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 2 : least_squares / corr > 0.1 features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 3 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 4 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w / poly, degree=1\n",
    "\n",
    "Test 5 : least_squares / all features standardized / y = -1,1 / random init_w / median + categorical\n",
    "\n",
    "Test 6 : logistic_regressoin  /all features standardized / y = 0,1 / random init_w / median + categorical + balanced\n",
    "\n",
    "Test 7 : least_squares / all features standardized / y = 0,1 / random init_w\n",
    "\n",
    "Test 8 : Test 1\n",
    "\n",
    "Test 9 : Test 1\n",
    "\n",
    "Test 10 : Test 1 / standardized test_set with mean and std from train_set\n",
    "\n",
    "Test 11 : Test 1 / standardized test_set with mean and std from train_set / balance\n",
    "\n",
    "Test 12 : Ridge regression / non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 13 : Ridge regression / balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 14 : Ridge regression / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 15 : Ridge regression / Removed all rows containing at least a NaN / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11 / Replaced NaN values in test_set by median in test_set\n",
    "\n",
    "Test 16 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Ensembling with: (\"least_squares_GD\", 1, 150, 0, 0.01, 0), (\"least_squares_GD\", 1, 50, 0, 0.25, 0), (\"least_squares_SGD\", 1, 30, 256, 0.2, 0), (\"least_squares_SGD\", 1, 60, 64, 0.1, 0), (\"ridge_regression\", 7, 0, 0, 0, 0.001), (\"ridge_regression\", 9, 0, 0, 0, 0.001), (\"ridge_regression\", 11, 0, 0, 0, 0.001)\n",
    "\n",
    "Test 17 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 11, 11, 11] and lambdas = [0.00046415888336127773, 0.001291549665014884, 4.6415888336127818e-05, 1.0000000000000001e-05]\n",
    "\n",
    "Test 18 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 12, 12, 11] and lambdas = [4.6415888336127818e-05, 0.001, 0.0021544346900318821, 2.1544346900318823e-05]\n",
    "\n",
    "Test 19 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 12, 12, 11] and lambdas = [2.1544346900318823e-05, 0.0021544346900318821, 0.00046415888336127773, 0.0001]\n",
    "\n",
    "Test 20 : balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 13, 12, 10] and lambdas = [0.001, 0.001, 0.0021544346900318821, 0.001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- balance output (batch numpy)\n",
    "- median and category\n",
    "- features engineering : features d'int√©raction\n",
    "- logistic regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
