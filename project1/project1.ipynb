{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOWER_BOUND = -1\n",
    "UPPER_BOUND = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"competition-data/\"\n",
    "DATA_TEST = \"test.csv\"\n",
    "DATA_TRAIN = \"train.csv\"\n",
    "\n",
    "y_train, x_train, ids_train = load_csv_data(DATA_FOLDER + DATA_TRAIN, LOWER_BOUND, UPPER_BOUND)\n",
    "y_test, x_test, ids_test = load_csv_data(DATA_FOLDER + DATA_TEST, LOWER_BOUND, UPPER_BOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_train = x_train.T\n",
    "tx_test = x_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 138.47 ,  160.937,      nan, ...,  105.457,   94.951,      nan],\n",
       "       [  51.655,   68.768,  162.172, ...,   60.526,   19.362,   72.756],\n",
       "       [  97.827,  103.235,  125.953, ...,   75.839,   68.812,   70.831],\n",
       "       ..., \n",
       "       [   1.24 ,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "       [  -2.475,      nan,      nan, ...,      nan,      nan,      nan],\n",
       "       [ 113.497,   46.226,   44.251, ...,   41.992,    0.   ,    0.   ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 568238)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train, idx_to_del = delete_features(tx_train, 0.7)\n",
    "tx_test = delete_features_from_idx(tx_test, idx_to_del)\n",
    "tx_train.shape\n",
    "tx_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 12, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_train = replace_nan_by_median(tx_train)\n",
    "tx_test = replace_nan_by_median(tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_train, std_train, tx_train = standardize(tx_train)\n",
    "tx_test = standardize_predef(tx_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tx_train = min_max(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_train, y_train = balance(tx_train.T, y_train, LOWER_BOUND, UPPER_BOUND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 250000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_train\n",
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation for Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6d85a02aac403fbc439899d51857c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: (1, 0, 0.0001, 0.7338039999999999)\n",
      "Finished: (1, 0, 0.00021544346900318845, 0.73377600000000009)\n",
      "Finished: (1, 0, 0.00046415888336127773, 0.73373199999999994)\n",
      "Finished: (1, 0, 0.001, 0.73372800000000005)\n",
      "Finished: (1, 0, 0.0021544346900318821, 0.73361999999999994)\n",
      "Finished: (1, 0, 0.0046415888336127772, 0.73359200000000002)\n",
      "Finished: (1, 0, 0.01, 0.73314799999999991)\n",
      "Finished: (1, 0, 0.021544346900318822, 0.73220400000000008)\n",
      "Finished: (1, 0, 0.046415888336127774, 0.72988799999999998)\n",
      "Finished: (1, 0, 0.10000000000000001, 0.72628800000000004)\n",
      "Finished: (2, 0, 0.0001, 0.7656639999999999)\n",
      "Finished: (2, 0, 0.00021544346900318845, 0.76570799999999983)\n",
      "Finished: (2, 0, 0.00046415888336127773, 0.76570000000000005)\n",
      "Finished: (2, 0, 0.001, 0.76567599999999991)\n",
      "Finished: (2, 0, 0.0021544346900318821, 0.76539200000000007)\n",
      "Finished: (2, 0, 0.0046415888336127772, 0.76514800000000005)\n",
      "Finished: (2, 0, 0.01, 0.76461999999999997)\n",
      "Finished: (2, 0, 0.021544346900318822, 0.76370800000000005)\n",
      "Finished: (2, 0, 0.046415888336127774, 0.76160800000000006)\n",
      "Finished: (2, 0, 0.10000000000000001, 0.7586440000000001)\n",
      "Finished: (3, 0, 0.0001, 0.77853600000000001)\n",
      "Finished: (3, 0, 0.00021544346900318845, 0.77857199999999982)\n",
      "Finished: (3, 0, 0.00046415888336127773, 0.77854000000000001)\n",
      "Finished: (3, 0, 0.001, 0.77849600000000008)\n",
      "Finished: (3, 0, 0.0021544346900318821, 0.77838000000000007)\n",
      "Finished: (3, 0, 0.0046415888336127772, 0.77820800000000001)\n",
      "Finished: (3, 0, 0.01, 0.77736799999999995)\n",
      "Finished: (3, 0, 0.021544346900318822, 0.77606800000000009)\n",
      "Finished: (3, 0, 0.046415888336127774, 0.77448800000000007)\n",
      "Finished: (3, 0, 0.10000000000000001, 0.77229999999999999)\n",
      "Finished: (4, 0, 0.0001, 0.78682800000000008)\n",
      "Finished: (4, 0, 0.00021544346900318845, 0.78678400000000004)\n",
      "Finished: (4, 0, 0.00046415888336127773, 0.78671199999999997)\n",
      "Finished: (4, 0, 0.001, 0.78671999999999997)\n",
      "Finished: (4, 0, 0.0021544346900318821, 0.78642400000000001)\n",
      "Finished: (4, 0, 0.0046415888336127772, 0.78632399999999991)\n",
      "Finished: (4, 0, 0.01, 0.78581999999999996)\n",
      "Finished: (4, 0, 0.021544346900318822, 0.78500400000000004)\n",
      "Finished: (4, 0, 0.046415888336127774, 0.78359999999999996)\n",
      "Finished: (4, 0, 0.10000000000000001, 0.78073599999999987)\n",
      "Finished: (5, 0, 0.0001, 0.79035199999999994)\n",
      "Finished: (5, 0, 0.00021544346900318845, 0.79035999999999995)\n",
      "Finished: (5, 0, 0.00046415888336127773, 0.79035200000000005)\n",
      "Finished: (5, 0, 0.001, 0.79015200000000019)\n",
      "Finished: (5, 0, 0.0021544346900318821, 0.79005199999999998)\n",
      "Finished: (5, 0, 0.0046415888336127772, 0.78971999999999998)\n",
      "Finished: (5, 0, 0.01, 0.78938799999999998)\n",
      "Finished: (5, 0, 0.021544346900318822, 0.78843200000000002)\n",
      "Finished: (5, 0, 0.046415888336127774, 0.78710799999999992)\n",
      "Finished: (5, 0, 0.10000000000000001, 0.78299600000000003)\n",
      "Finished: (6, 0, 0.0001, 0.79237200000000008)\n",
      "Finished: (6, 0, 0.00021544346900318845, 0.79228799999999988)\n",
      "Finished: (6, 0, 0.00046415888336127773, 0.792292)\n",
      "Finished: (6, 0, 0.001, 0.79224399999999995)\n",
      "Finished: (6, 0, 0.0021544346900318821, 0.79211600000000004)\n",
      "Finished: (6, 0, 0.0046415888336127772, 0.791744)\n",
      "Finished: (6, 0, 0.01, 0.79148399999999997)\n",
      "Finished: (6, 0, 0.021544346900318822, 0.79037199999999996)\n",
      "Finished: (6, 0, 0.046415888336127774, 0.78767200000000004)\n",
      "Finished: (6, 0, 0.10000000000000001, 0.78367199999999992)\n",
      "Finished: (7, 0, 0.0001, 0.79874400000000001)\n",
      "Finished: (7, 0, 0.00021544346900318845, 0.79870399999999997)\n",
      "Finished: (7, 0, 0.00046415888336127773, 0.79867999999999995)\n",
      "Finished: (7, 0, 0.001, 0.79856400000000005)\n",
      "Finished: (7, 0, 0.0021544346900318821, 0.79844799999999994)\n",
      "Finished: (7, 0, 0.0046415888336127772, 0.79785600000000012)\n",
      "Finished: (7, 0, 0.01, 0.79682399999999998)\n",
      "Finished: (7, 0, 0.021544346900318822, 0.79540799999999989)\n",
      "Finished: (7, 0, 0.046415888336127774, 0.79200800000000005)\n",
      "Finished: (7, 0, 0.10000000000000001, 0.78665600000000002)\n",
      "Finished: (8, 0, 0.0001, 0.80580399999999996)\n",
      "Finished: (8, 0, 0.00021544346900318845, 0.78391600000000006)\n",
      "Finished: (8, 0, 0.00046415888336127773, 0.80592799999999998)\n",
      "Finished: (8, 0, 0.001, 0.80611200000000005)\n",
      "Finished: (8, 0, 0.0021544346900318821, 0.80585200000000001)\n",
      "Finished: (8, 0, 0.0046415888336127772, 0.80523600000000006)\n",
      "Finished: (8, 0, 0.01, 0.80350800000000011)\n",
      "Finished: (8, 0, 0.021544346900318822, 0.80201999999999996)\n",
      "Finished: (8, 0, 0.046415888336127774, 0.79735599999999995)\n",
      "Finished: (8, 0, 0.10000000000000001, 0.79014399999999996)\n",
      "Finished: (9, 0, 0.0001, 0.81082799999999988)\n",
      "Finished: (9, 0, 0.00021544346900318845, 0.81077600000000005)\n",
      "Finished: (9, 0, 0.00046415888336127773, 0.81069200000000008)\n",
      "Finished: (9, 0, 0.001, 0.81064000000000003)\n",
      "Finished: (9, 0, 0.0021544346900318821, 0.81010000000000004)\n",
      "Finished: (9, 0, 0.0046415888336127772, 0.80969200000000008)\n",
      "Finished: (9, 0, 0.01, 0.8086040000000001)\n",
      "Finished: (9, 0, 0.021544346900318822, 0.80594799999999989)\n",
      "Finished: (9, 0, 0.046415888336127774, 0.80041999999999991)\n",
      "Finished: (9, 0, 0.10000000000000001, 0.79125199999999996)\n",
      "Finished: (10, 0, 0.0001, 0.81158400000000008)\n",
      "Finished: (10, 0, 0.00021544346900318845, 0.81155200000000005)\n",
      "Finished: (10, 0, 0.00046415888336127773, 0.81136399999999997)\n",
      "Finished: (10, 0, 0.001, 0.81148400000000009)\n",
      "Finished: (10, 0, 0.0021544346900318821, 0.81108800000000003)\n",
      "Finished: (10, 0, 0.0046415888336127772, 0.80960800000000011)\n",
      "Finished: (10, 0, 0.01, 0.81003199999999997)\n",
      "Finished: (10, 0, 0.021544346900318822, 0.80767600000000006)\n",
      "Finished: (10, 0, 0.046415888336127774, 0.80079600000000006)\n",
      "Finished: (10, 0, 0.10000000000000001, 0.79322400000000004)\n",
      "Finished: (11, 0, 0.0001, 0.81307200000000002)\n",
      "Finished: (11, 0, 0.00021544346900318845, 0.81306800000000001)\n",
      "Finished: (11, 0, 0.00046415888336127773, 0.81297599999999992)\n",
      "Finished: (11, 0, 0.001, 0.81299200000000005)\n",
      "Finished: (11, 0, 0.0021544346900318821, 0.812724)\n",
      "Finished: (11, 0, 0.0046415888336127772, 0.80998400000000004)\n",
      "Finished: (11, 0, 0.01, 0.81114799999999998)\n",
      "Finished: (11, 0, 0.021544346900318822, 0.80840400000000001)\n",
      "Finished: (11, 0, 0.046415888336127774, 0.8034039999999999)\n",
      "Finished: (11, 0, 0.10000000000000001, 0.79440000000000011)\n",
      "Finished: (12, 0, 0.0001, 0.786524)\n",
      "Finished: (12, 0, 0.00021544346900318845, 0.80345200000000006)\n",
      "Finished: (12, 0, 0.00046415888336127773, 0.80970799999999998)\n",
      "Finished: (12, 0, 0.001, 0.79500800000000005)\n",
      "Finished: (12, 0, 0.0021544346900318821, 0.78279199999999993)\n",
      "Finished: (12, 0, 0.0046415888336127772, 0.79458000000000006)\n",
      "Finished: (12, 0, 0.01, 0.81098000000000003)\n",
      "Finished: (12, 0, 0.021544346900318822, 0.80934400000000006)\n",
      "Finished: (12, 0, 0.046415888336127774, 0.80342800000000003)\n",
      "Finished: (12, 0, 0.10000000000000001, 0.78961599999999998)\n",
      "\n",
      "[(1, 0, 0.0001, 0.7338039999999999), (1, 0, 0.00021544346900318845, 0.73377600000000009), (1, 0, 0.00046415888336127773, 0.73373199999999994), (1, 0, 0.001, 0.73372800000000005), (1, 0, 0.0021544346900318821, 0.73361999999999994), (1, 0, 0.0046415888336127772, 0.73359200000000002), (1, 0, 0.01, 0.73314799999999991), (1, 0, 0.021544346900318822, 0.73220400000000008), (1, 0, 0.046415888336127774, 0.72988799999999998), (1, 0, 0.10000000000000001, 0.72628800000000004), (2, 0, 0.0001, 0.7656639999999999), (2, 0, 0.00021544346900318845, 0.76570799999999983), (2, 0, 0.00046415888336127773, 0.76570000000000005), (2, 0, 0.001, 0.76567599999999991), (2, 0, 0.0021544346900318821, 0.76539200000000007), (2, 0, 0.0046415888336127772, 0.76514800000000005), (2, 0, 0.01, 0.76461999999999997), (2, 0, 0.021544346900318822, 0.76370800000000005), (2, 0, 0.046415888336127774, 0.76160800000000006), (2, 0, 0.10000000000000001, 0.7586440000000001), (3, 0, 0.0001, 0.77853600000000001), (3, 0, 0.00021544346900318845, 0.77857199999999982), (3, 0, 0.00046415888336127773, 0.77854000000000001), (3, 0, 0.001, 0.77849600000000008), (3, 0, 0.0021544346900318821, 0.77838000000000007), (3, 0, 0.0046415888336127772, 0.77820800000000001), (3, 0, 0.01, 0.77736799999999995), (3, 0, 0.021544346900318822, 0.77606800000000009), (3, 0, 0.046415888336127774, 0.77448800000000007), (3, 0, 0.10000000000000001, 0.77229999999999999), (4, 0, 0.0001, 0.78682800000000008), (4, 0, 0.00021544346900318845, 0.78678400000000004), (4, 0, 0.00046415888336127773, 0.78671199999999997), (4, 0, 0.001, 0.78671999999999997), (4, 0, 0.0021544346900318821, 0.78642400000000001), (4, 0, 0.0046415888336127772, 0.78632399999999991), (4, 0, 0.01, 0.78581999999999996), (4, 0, 0.021544346900318822, 0.78500400000000004), (4, 0, 0.046415888336127774, 0.78359999999999996), (4, 0, 0.10000000000000001, 0.78073599999999987), (5, 0, 0.0001, 0.79035199999999994), (5, 0, 0.00021544346900318845, 0.79035999999999995), (5, 0, 0.00046415888336127773, 0.79035200000000005), (5, 0, 0.001, 0.79015200000000019), (5, 0, 0.0021544346900318821, 0.79005199999999998), (5, 0, 0.0046415888336127772, 0.78971999999999998), (5, 0, 0.01, 0.78938799999999998), (5, 0, 0.021544346900318822, 0.78843200000000002), (5, 0, 0.046415888336127774, 0.78710799999999992), (5, 0, 0.10000000000000001, 0.78299600000000003), (6, 0, 0.0001, 0.79237200000000008), (6, 0, 0.00021544346900318845, 0.79228799999999988), (6, 0, 0.00046415888336127773, 0.792292), (6, 0, 0.001, 0.79224399999999995), (6, 0, 0.0021544346900318821, 0.79211600000000004), (6, 0, 0.0046415888336127772, 0.791744), (6, 0, 0.01, 0.79148399999999997), (6, 0, 0.021544346900318822, 0.79037199999999996), (6, 0, 0.046415888336127774, 0.78767200000000004), (6, 0, 0.10000000000000001, 0.78367199999999992), (7, 0, 0.0001, 0.79874400000000001), (7, 0, 0.00021544346900318845, 0.79870399999999997), (7, 0, 0.00046415888336127773, 0.79867999999999995), (7, 0, 0.001, 0.79856400000000005), (7, 0, 0.0021544346900318821, 0.79844799999999994), (7, 0, 0.0046415888336127772, 0.79785600000000012), (7, 0, 0.01, 0.79682399999999998), (7, 0, 0.021544346900318822, 0.79540799999999989), (7, 0, 0.046415888336127774, 0.79200800000000005), (7, 0, 0.10000000000000001, 0.78665600000000002), (8, 0, 0.0001, 0.80580399999999996), (8, 0, 0.00021544346900318845, 0.78391600000000006), (8, 0, 0.00046415888336127773, 0.80592799999999998), (8, 0, 0.001, 0.80611200000000005), (8, 0, 0.0021544346900318821, 0.80585200000000001), (8, 0, 0.0046415888336127772, 0.80523600000000006), (8, 0, 0.01, 0.80350800000000011), (8, 0, 0.021544346900318822, 0.80201999999999996), (8, 0, 0.046415888336127774, 0.79735599999999995), (8, 0, 0.10000000000000001, 0.79014399999999996), (9, 0, 0.0001, 0.81082799999999988), (9, 0, 0.00021544346900318845, 0.81077600000000005), (9, 0, 0.00046415888336127773, 0.81069200000000008), (9, 0, 0.001, 0.81064000000000003), (9, 0, 0.0021544346900318821, 0.81010000000000004), (9, 0, 0.0046415888336127772, 0.80969200000000008), (9, 0, 0.01, 0.8086040000000001), (9, 0, 0.021544346900318822, 0.80594799999999989), (9, 0, 0.046415888336127774, 0.80041999999999991), (9, 0, 0.10000000000000001, 0.79125199999999996), (10, 0, 0.0001, 0.81158400000000008), (10, 0, 0.00021544346900318845, 0.81155200000000005), (10, 0, 0.00046415888336127773, 0.81136399999999997), (10, 0, 0.001, 0.81148400000000009), (10, 0, 0.0021544346900318821, 0.81108800000000003), (10, 0, 0.0046415888336127772, 0.80960800000000011), (10, 0, 0.01, 0.81003199999999997), (10, 0, 0.021544346900318822, 0.80767600000000006), (10, 0, 0.046415888336127774, 0.80079600000000006), (10, 0, 0.10000000000000001, 0.79322400000000004), (11, 0, 0.0001, 0.81307200000000002), (11, 0, 0.00021544346900318845, 0.81306800000000001), (11, 0, 0.00046415888336127773, 0.81297599999999992), (11, 0, 0.001, 0.81299200000000005), (11, 0, 0.0021544346900318821, 0.812724), (11, 0, 0.0046415888336127772, 0.80998400000000004), (11, 0, 0.01, 0.81114799999999998), (11, 0, 0.021544346900318822, 0.80840400000000001), (11, 0, 0.046415888336127774, 0.8034039999999999), (11, 0, 0.10000000000000001, 0.79440000000000011), (12, 0, 0.0001, 0.786524), (12, 0, 0.00021544346900318845, 0.80345200000000006), (12, 0, 0.00046415888336127773, 0.80970799999999998), (12, 0, 0.001, 0.79500800000000005), (12, 0, 0.0021544346900318821, 0.78279199999999993), (12, 0, 0.0046415888336127772, 0.79458000000000006), (12, 0, 0.01, 0.81098000000000003), (12, 0, 0.021544346900318822, 0.80934400000000006), (12, 0, 0.046415888336127774, 0.80342800000000003), (12, 0, 0.10000000000000001, 0.78961599999999998)]\n",
      "Best accuracy:  0.813072\n",
      "Best degree:  11\n",
      "Best gamma:  0\n",
      "Best lambda:  0.0001\n"
     ]
    }
   ],
   "source": [
    "model = \"ridge_regression\"\n",
    "seed = 3\n",
    "k_fold = 10\n",
    "k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "degrees = np.arange(1, 13, 1)\n",
    "lambdas = np.logspace(-4, -1, 10)\n",
    "gammas = [0]\n",
    "max_iters = 0\n",
    "\n",
    "def cross_validation_demo(model, k_fold, k_indices, degrees, lambdas, gammas, max_iters, tx_train, y_train):\n",
    "    results = []\n",
    "    for degree in tqdm_notebook(degrees):\n",
    "        tx_poly = build_poly_tx(tx_train, degree)\n",
    "        initial_w = init_w(tx_poly)\n",
    "        for gamma in gammas:\n",
    "            for lambda_ in lambdas:\n",
    "                accs = []\n",
    "                ws = []\n",
    "                for k in range(k_fold):\n",
    "                    w_tr, acc = cross_validation(y_train, tx_poly.T, initial_w,\n",
    "                                                 max_iters,  k_indices, k, gamma, lambda_, LOWER_BOUND, UPPER_BOUND, model)\n",
    "                    ws.append(w_tr)\n",
    "                    accs.append(acc)\n",
    "                w_final = np.mean(ws, axis=0)\n",
    "                acc_mean = np.mean(accs)\n",
    "                results.append((degree, gamma, lambda_, acc_mean))\n",
    "\n",
    "                print(\"Finished: \" + str((degree, gamma, lambda_, acc_mean)))\n",
    "    \n",
    "    print(results)\n",
    "    \n",
    "    best_degree, best_gamma, best_lambda, best_acc_mean = max(results, key=lambda x: x[3])\n",
    "    print(\"Best accuracy: \",best_acc_mean)\n",
    "    print(\"Best degree: \", best_degree)\n",
    "    print(\"Best gamma: \", best_gamma)\n",
    "    print(\"Best lambda: \", best_lambda)\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = cross_validation_demo(model, k_fold, k_indices, degrees, lambdas, gammas, max_iters, tx_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.7, 0, 0.75813200000000003)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joel/anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py:4243: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  sy = (ymax - ymin) / ny\n",
      "/Users/Joel/anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py:4346: RuntimeWarning: invalid value encountered in multiply\n",
      "  offsets[:, 1] *= sy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAF3CAYAAABe2rRKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAWiffLUlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b7de470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# show the best results for different parameters of the ridge regression\n",
    "def show_ridge_results(results, l_degrees):\n",
    "\n",
    "    df = pd.DataFrame.from_records(results, columns=['degree', 'gamma', 'lambda', 'acc'])\n",
    "    df.plot.hexbin(x='lambda', y='degree', C='acc', gridsize=l_degrees, figsize=(10,6), sharex=False, cmap='jet')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "    \n",
    "df = show_ridge_results(results, len(degrees))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.13681128],\n",
       "        [-0.09585492],\n",
       "        [ 0.03103595],\n",
       "        ..., \n",
       "        [-0.34341299],\n",
       "        [-0.06824147],\n",
       "        [-0.49605018]]), array([[ 0.1332059 ],\n",
       "        [-0.097928  ],\n",
       "        [ 0.05540158],\n",
       "        ..., \n",
       "        [-0.31900422],\n",
       "        [-0.05139256],\n",
       "        [-0.49016231]]), array([[ 0.14684682],\n",
       "        [-0.0766211 ],\n",
       "        [-0.02604242],\n",
       "        ..., \n",
       "        [-0.34806072],\n",
       "        [-0.07771574],\n",
       "        [-0.48895453]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_raw_predictions(results, degrees, tx, y):\n",
    "    y_raw_preds = []\n",
    "    ws = []\n",
    "    for d in degrees:\n",
    "        r_d = [r for r in results if r[0] == d]\n",
    "        degree, gamma, lambda_, acc_mean = max(r_d, key=lambda x: x[3])\n",
    "        tx_poly = build_poly_tx(tx, d)\n",
    "        w, l = ridge_regression(y, tx_poly, lambda_)\n",
    "        ws.append(w)\n",
    "        y_raw_pred = np.dot(tx_poly.T, np.array(w))\n",
    "        y_raw_preds.append(y_raw_pred)\n",
    "        \n",
    "    return y_raw_preds, ws\n",
    "\n",
    "y_raw_preds, ws = compute_raw_predictions(results, [9, 10, 11], tx_train, y_train)\n",
    "y_raw_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13895467],\n",
       "       [-0.09013467],\n",
       "       [ 0.0201317 ],\n",
       "       ..., \n",
       "       [-0.33682598],\n",
       "       [-0.06578326],\n",
       "       [-0.49172234]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_raw_preds_mean = np.mean(y_raw_preds, axis=0)\n",
    "y_raw_preds_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [-1.]\n",
      " [ 1.]\n",
      " ..., \n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "threshold = (UPPER_BOUND + LOWER_BOUND)/2\n",
    "y_raw_preds_mean[np.where(y_raw_preds_mean <= threshold)] = -1\n",
    "y_raw_preds_mean[np.where(y_raw_preds_mean > threshold)] = 1\n",
    "print(y_raw_preds_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81234799999999996"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_train == y_raw_preds_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       ..., \n",
       "       [-1.],\n",
       "       [-1.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.13681128, -0.09585492,  0.03103595, ..., -0.34341299,\n",
       "        -0.06824147, -0.49605018],\n",
       "       [ 0.1332059 , -0.097928  ,  0.05540158, ..., -0.31900422,\n",
       "        -0.05139256, -0.49016231],\n",
       "       [ 0.14684682, -0.0766211 , -0.02604242, ..., -0.34806072,\n",
       "        -0.07771574, -0.48895453]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx = np.squeeze(y_raw_preds, axis=2)\n",
    "tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81234799999999996"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, l = least_squares(y_train, tx)\n",
    "\n",
    "y_t = np.dot(tx.T, w)\n",
    "\n",
    "np.mean(y_train == y_raw_preds_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6667ac1a6ee543aea179f4fbbaa46bd5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joel/Drive/EPFL/MA1/github/CS-433/project1/implementations.py:125: RuntimeWarning: overflow encountered in exp\n",
      "  exp_ = np.exp(tx.dot(w))\n",
      "/Users/Joel/Drive/EPFL/MA1/github/CS-433/project1/implementations.py:118: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: (1, 0.7, 0, 0.75813200000000003)\n",
      "\n",
      "[(1, 0.7, 0, 0.75813200000000003)]\n",
      "Best accuracy:  0.758132\n",
      "Best degree:  1\n",
      "Best gamma:  0.7\n",
      "Best lambda:  0\n"
     ]
    }
   ],
   "source": [
    "model = \"logistic_regression\"\n",
    "seed = 3\n",
    "k_fold = 4\n",
    "k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "degrees = [1]\n",
    "lambdas = [0]\n",
    "gammas = [0.7]\n",
    "max_iters = 1000\n",
    "\n",
    "def cross_validation_demo(model, k_fold, k_indices, degrees, lambdas, gammas, max_iters, tx_train, y_train):\n",
    "    results = []\n",
    "    for degree in tqdm_notebook(degrees):\n",
    "        tx_poly = build_poly_tx(tx_train, degree)\n",
    "        initial_w = init_w(tx_poly)\n",
    "        for gamma in gammas:\n",
    "            for lambda_ in lambdas:\n",
    "                accs = []\n",
    "                ws = []\n",
    "                for k in range(k_fold):\n",
    "                    w_tr, acc = cross_validation(y_train, tx_poly.T, initial_w,\n",
    "                                                 max_iters,  k_indices, k, gamma, lambda_, LOWER_BOUND, UPPER_BOUND, model)\n",
    "                    ws.append(w_tr)\n",
    "                    accs.append(acc)\n",
    "                w_final = np.mean(ws, axis=0)\n",
    "                acc_mean = np.mean(accs)\n",
    "                results.append((degree, gamma, lambda_, acc_mean))\n",
    "\n",
    "                print(\"Finished: \" + str((degree, gamma, lambda_, acc_mean)))\n",
    "    \n",
    "    print(results)\n",
    "    \n",
    "    best_degree, best_gamma, best_lambda, best_acc_mean = max(results, key=lambda x: x[3])\n",
    "    print(\"Best accuracy: \",best_acc_mean)\n",
    "    print(\"Best degree: \", best_degree)\n",
    "    print(\"Best gamma: \", best_gamma)\n",
    "    print(\"Best lambda: \", best_lambda)\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = cross_validation_demo(model, k_fold, k_indices, degrees, lambdas, gammas, max_iters, tx, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import predict_labels_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = predict_labels_kaggle(w_final, tx_test.T, LOWER_BOUND, UPPER_BOUND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import create_csv_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"test11.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test n : algorithm / features / y / w\n",
    "- - - - - - - - - - - - - - - - - - - \n",
    "Test 1 : least_squares / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 2 : least_squares / corr > 0.1 features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 3 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 4 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w / poly, degree=1\n",
    "\n",
    "Test 5 : least_squares / all features standardized / y = -1,1 / random init_w / median + categorical\n",
    "\n",
    "Test 6 : logistic_regressoin  /all features standardized / y = 0,1 / random init_w / median + categorical + balanced\n",
    "\n",
    "Test 7 : least_squares / all features standardized / y = 0,1 / random init_w\n",
    "\n",
    "Test 8 : Test 1\n",
    "\n",
    "Test 9 : Test 1\n",
    "\n",
    "Test 10 : Test 1 / standardized test_set with mean and std from train_set\n",
    "\n",
    "Test 11 : Test 1 / standardized test_set with mean and std from train_set / balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- balance output (batch numpy)\n",
    "- median and category\n",
    "- features engineering : features d'int√©raction\n",
    "- logistic regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
