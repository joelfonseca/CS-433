{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from implementations import *\n",
    "from proj1_helpers import create_csv_submission, load_csv_data, predict_labels_kaggle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = -1\n",
    "UPPER_BOUND = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"competition-data/\"\n",
    "DATA_TEST = \"test.csv\"\n",
    "DATA_TRAIN = \"train.csv\"\n",
    "y_train, x_train, ids_train = load_csv_data(DATA_FOLDER + DATA_TRAIN, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train = x_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inverse log values of features which are positive in value.\n",
    "inv_log_cols = [0, 2, 5, 7, 9, 10, 13, 16, 19, 21, 23, 26]\n",
    "\n",
    "tx_train_inv_log_cols = np.log(1 / (1 + tx_train[inv_log_cols]))\n",
    "tx_train = np.vstack((tx_train, tx_train_inv_log_cols))\n",
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot correlations between features\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i in range(29):\n",
    "    for j in range(i+1, 30):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        x = tx_train[:,tx_train[22] == 1][i]\n",
    "        y = tx_train[:,tx_train[22] == 1][j]\n",
    "        plt.xlabel('Feature %d' % i)\n",
    "        plt.ylabel('Feature %d' % j)\n",
    "        ax.scatter(x, y)\n",
    "        plt.show()\n",
    "\n",
    "'''\n",
    "for j in range(0, 30):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    x = tx_train[22]\n",
    "    y = tx_train[j]\n",
    "    plt.xlabel('Feature %d' % 22)\n",
    "    plt.ylabel('Feature %d' % j)\n",
    "    ax.scatter(x, y)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the data set to have the same number of 1 that -1 (downsample)\n",
    "'''\n",
    "tx_train, y_train = balance(tx_train.T, y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into 4 sets depending on the feature 22: 'PRI_jet_num' (categorical feature)\n",
    "masks_jet_train = get_jet_masks(tx_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace remaining NaN values with the median of the column + standardie the values of all features\n",
    "tx_train = replace_nan_by_median(tx_train)\n",
    "mean_train, std_train, tx_train = standardize(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST SET ###\n",
    "\n",
    "# Do the same we did to the training set, but to the test set\n",
    "y_test, x_test, ids_test = load_csv_data(DATA_FOLDER + DATA_TEST, LOWER_BOUND, UPPER_BOUND)\n",
    "masks_jet_test = get_jet_masks(x_test)\n",
    "tx_test = x_test.T\n",
    "\n",
    "tx_test_inv_log_cols = np.log(1 / (1 + tx_test[inv_log_cols]))\n",
    "tx_test = np.vstack((tx_test, tx_test_inv_log_cols))\n",
    "\n",
    "tx_test = replace_nan_by_median(tx_test)\n",
    "tx_test = standardize_predef(tx_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validate the different models one by one (change manually)\n",
    "model_nb = 3\n",
    "\n",
    "current_tx_train = tx_train.T[masks_jet_train[model_nb]].T\n",
    "current_y_train = y_train[masks_jet_train[model_nb]]\n",
    "\n",
    "### Those should be done if we choose not to do them before separating the data set\n",
    "'''\n",
    "# Remove columns full of NaN\n",
    "current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "\n",
    "# Remove columns without standard deviation at all\n",
    "current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "\n",
    "# Balance the data set\n",
    "#current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "# Replace remaining NaN by median\n",
    "current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "\n",
    "# Standardize features\n",
    "mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "'''\n",
    "print(current_tx_train.shape)\n",
    "print(current_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import build_k_indices\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def cross_validation_demo():\n",
    "    model = \"ridge_regression\"\n",
    "    seed = 3\n",
    "    k_fold = 10\n",
    "    degrees = np.arange(5, 13, 1)\n",
    "    lambdas = np.logspace(-5, -2, 10)\n",
    "    gammas = [0]#np.arange(0.05, 0.6, 0.05)\n",
    "    #initial_w = init_w(tx_train)\n",
    "    max_iters = [50]#np.logspace(2, 3, 4)\n",
    "    batch_sizes = [64]\n",
    "    k_indices = build_k_indices(current_y_train, k_fold, seed)\n",
    "    results = []\n",
    "    for degree in tqdm_notebook(degrees):\n",
    "        tx_train_poly = build_poly_tx(current_tx_train, degree)\n",
    "        initial_w = init_w(tx_train_poly)\n",
    "        for max_iter in max_iters:\n",
    "            for batch_size in batch_sizes:\n",
    "                for gamma in gammas:\n",
    "                    for i, lambda_ in enumerate(lambdas):\n",
    "                        accs = []\n",
    "                        ws = []\n",
    "                        for k in range(k_fold):\n",
    "                            w_tr, acc = cross_validation(current_y_train, tx_train_poly.T, initial_w,\n",
    "                                                         int(max_iter), k_indices, k, gamma, lambda_, LOWER_BOUND, UPPER_BOUND, model, batch_size)\n",
    "                            ws.append(w_tr)\n",
    "                            accs.append(acc)\n",
    "                        w_mean = np.mean(ws, axis=0)\n",
    "                        acc_mean = np.mean(accs)\n",
    "                        results.append((degree, max_iter, batch_size, gamma, lambda_, acc_mean, w_mean, i))\n",
    "\n",
    "                        print(\"Finished: \" + str((degree, max_iter, batch_size, gamma, lambda_, acc_mean)))\n",
    "\n",
    "                        '''\n",
    "                        fig = plt.subplots(1, 1, figsize=(10,5))\n",
    "                        plt.plot(range(1,k_fold+1), accs, marker=\".\", color='b', label='accuracy')\n",
    "                        plt.axhline(y=acc_mean, color='r', label='mean')\n",
    "                        ax = plt.gca()\n",
    "                        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                        plt.xlabel(\"k\")\n",
    "                        plt.ylabel(\"accuracy\")\n",
    "                        plt.title(\"k-fold accuracy for lambda=%.5f, gamma=%.2f, degree=%d\" % (lambda_, gamma, degree))\n",
    "                        plt.legend(loc=2)\n",
    "                        plt.grid(True)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = show_ridge_results(results, 8, \"model_nb_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort(key=lambda x: -x[5])\n",
    "[x[4] for x in results[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating into multiple models\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 0, 0, 0, 0.00046415888336127773, 0.81200248756218907)\n",
    "* For 1: Best is Ridge (11, 0, 0, 0, 0.001291549665014884, 0.79104143337066068)\n",
    "* For 2: Best is Ridge (11, 50, 64, 0, 4.6415888336127818e-05, 0.84399274987053341)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 1.0000000000000001e-05, 0.80668918918918919)\n",
    "\n",
    "With balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 0.001, 0.81080816006276968)\n",
    "* For 1: Best is Ridge (13, 50, 64, 0, 0.001, 0.79271021291952359)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83351592615134906)\n",
    "* For 3: Best is Ridge (10, 50, 64, 0, 0.001, 0.80564635958395248)\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (11, 50, 64, 0, 0.0001, 0.81401432575521648)\n",
    "* For 1: Best is Ridge (11, 50, 64, 0, 3.1622776601683795e-05, 0.79476234855545214)\n",
    "* For 2: Best is Ridge (10, 50, 64, 0, 0.00031622776601683794, 0.84767954368680321)\n",
    "* For 3: Best is Ridge (9, 50, 64, 0, 0.01, 0.80967741935483883)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 50, 64, 0, 4.6415888336127818e-05, 0.84248823941547391)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.001, 0.80530049006964144)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83388921977367492)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 2.1544346900318823e-05, 0.82996389891696754)\n",
    "* For 2-3: Best is Ridge (11, 50, 64, 0, 0.001, 0.82996967190515569)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 2.1544346900318823e-05, 0.84242818536683006)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.80531338663915408)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.83325392098471307)\n",
    "* For 3: Best is Ridge (12, 50, 64, 0, 0.0001, 0.8336642599277978)\n",
    "* For 2-3: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.83195478356768682)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (10, 50, 64, 0, 0.00021544346900318823, 0.84473025723150852)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.80794428681970598)\n",
    "* For 2: Best is Ridge (9, 50, 64, 0, 1.0000000000000001e-05, 0.83732380385149896)\n",
    "* For 3: Best is Ridge (10, 50, 64, 0, 0.001, 0.83948555956678705)\n",
    "* For 2-3: Best is Ridge (10, 50, 64, 0, 0.001, 0.83607664736696985)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (9, 50, 64, 0, 4.6415888336127818e-05, 0.84425983385046544)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.80806035594531855)\n",
    "* For 2-3: Best is Ridge (10, 50, 64, 0, 0.00046415888336127773, 0.83569065343258886)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code to use all models on the test_set and have the final prediction in y_pred\n",
    "\n",
    "degrees = [10, 12, 9, 10]\n",
    "lambdas = [0.00021544346900318823, 0.00046415888336127773, 1.0000000000000001e-05, 0.001]\n",
    "\n",
    "# Final prediction in here\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "for i in range(len(masks_jet_train)):\n",
    "    current_tx_train = tx_train.T[masks_jet_train[i]].T\n",
    "    current_tx_test = tx_test.T[masks_jet_test[i]].T\n",
    "    current_y_train = y_train[masks_jet_train[i]]\n",
    "    \n",
    "    # Preprocess here if not preprocessed before separating\n",
    "    '''\n",
    "    # Remove columns full of NaN\n",
    "    current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "    current_tx_test = current_tx_test[~np.all(np.isnan(current_tx_test), axis=1)]\n",
    "\n",
    "    # Remove columns without standard deviation at all\n",
    "    current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "    current_tx_test = current_tx_test[np.nanstd(current_tx_test, axis=1) != 0]\n",
    "    \n",
    "    # Balance the data set\n",
    "    #current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "    # Replace remaining NaN by median\n",
    "    current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "    current_tx_test = replace_nan_by_median(current_tx_test)\n",
    "\n",
    "    # Standardize features\n",
    "    mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "    current_tx_test = standardize_predef(current_tx_test, mean_train, std_train)\n",
    "    '''\n",
    "    # Build poly\n",
    "    current_tx_poly_train = build_poly_tx(current_tx_train, degrees[i])\n",
    "    current_tx_poly_test = build_poly_tx(current_tx_test, degrees[i])\n",
    "    \n",
    "    # Compute best method\n",
    "    current_w, current_loss = ridge_regression(current_y_train, current_tx_poly_train, lambdas[i])\n",
    "    \n",
    "    acc = accuracy(current_y_train, current_tx_poly_train.T, current_w, LOWER_BOUND, UPPER_BOUND)\n",
    "    \n",
    "    print(\"Accuracy:\", acc)\n",
    "    \n",
    "    # Predict\n",
    "    y_test_pred = predict_labels_kaggle(current_w, current_tx_poly_test.T, LOWER_BOUND, UPPER_BOUND)\n",
    "    y_pred[masks_jet_test[i]] = y_test_pred.flatten()\n",
    "\n",
    "print(\"Number of %d:\" % UPPER_BOUND, np.count_nonzero(y_pred == UPPER_BOUND))\n",
    "print(\"Number of %d:\" % LOWER_BOUND, np.count_nonzero(y_pred == LOWER_BOUND))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"test25.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test n : algorithm / features / y / w\n",
    "- - - - - - - - - - - - - - - - - - - \n",
    "Test 1 : least_squares / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 2 : least_squares / corr > 0.1 features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 3 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 4 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w / poly, degree=1\n",
    "\n",
    "Test 5 : least_squares / all features standardized / y = -1,1 / random init_w / median + categorical\n",
    "\n",
    "Test 6 : logistic_regressoin  /all features standardized / y = 0,1 / random init_w / median + categorical + balanced\n",
    "\n",
    "Test 7 : least_squares / all features standardized / y = 0,1 / random init_w\n",
    "\n",
    "Test 8 : Test 1\n",
    "\n",
    "Test 9 : Test 1\n",
    "\n",
    "Test 10 : Test 1 / standardized test_set with mean and std from train_set\n",
    "\n",
    "Test 11 : Test 1 / standardized test_set with mean and std from train_set / balance\n",
    "\n",
    "Test 12 : Ridge regression / non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 13 : Ridge regression / balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 14 : Ridge regression / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 15 : Ridge regression / Removed all rows containing at least a NaN / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11 / Replaced NaN values in test_set by median in test_set\n",
    "\n",
    "Test 16 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Ensembling with: (\"least_squares_GD\", 1, 150, 0, 0.01, 0), (\"least_squares_GD\", 1, 50, 0, 0.25, 0), (\"least_squares_SGD\", 1, 30, 256, 0.2, 0), (\"least_squares_SGD\", 1, 60, 64, 0.1, 0), (\"ridge_regression\", 7, 0, 0, 0, 0.001), (\"ridge_regression\", 9, 0, 0, 0, 0.001), (\"ridge_regression\", 11, 0, 0, 0, 0.001)\n",
    "\n",
    "Test 17 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 11, 11, 11] and lambdas = [0.00046415888336127773, 0.001291549665014884, 4.6415888336127818e-05, 1.0000000000000001e-05]\n",
    "\n",
    "Test 18 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 12, 12, 11] and lambdas = [4.6415888336127818e-05, 0.001, 0.0021544346900318821, 2.1544346900318823e-05]\n",
    "\n",
    "Test 19 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 12, 12, 11] and lambdas = [2.1544346900318823e-05, 0.0021544346900318821, 0.00046415888336127773, 0.0001]\n",
    "\n",
    "Test 20 : balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 13, 12, 10] and lambdas = [0.001, 0.001, 0.0021544346900318821, 0.001]\n",
    "\n",
    "Test 21 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [8, 12, 12] and lambdas = [2.1544346900318823e-05, 0.0021544346900318821, 0.00046415888336127773]\n",
    "\n",
    "Test 22 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [9, 12, 10] and lambdas = [4.6415888336127818e-05, 0.0021544346900318821, 0.00046415888336127773] / New features being inverse log\n",
    "\n",
    "Test 23 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [10, 12, 10] and lambdas = [0.00021544346900318823, 0.00046415888336127773, 0.001] / New features being inverse log\n",
    "\n",
    "Test 24 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [10, 12, 9, 10] and lambdas = [0.00021544346900318823, 0.00046415888336127773, 1.0000000000000001e-05, 0.001] / New features being inverse log\n",
    "\n",
    "Test 25 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [11, 11, 10, 9] and lambdas = [0.0001, 3.1622776601683795e-05, 0.00031622776601683794, 0.01] / New features being inverse log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- balance output (batch numpy)\n",
    "- median and category\n",
    "- features engineering : features d'intéraction\n",
    "- logistic regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
