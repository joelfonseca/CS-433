{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from implementations import *\n",
    "from proj1_helpers import create_csv_submission, load_csv_data, predict_labels_kaggle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = -1\n",
    "UPPER_BOUND = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"competition-data/\"\n",
    "DATA_TEST = \"test.csv\"\n",
    "DATA_TRAIN = \"train.csv\"\n",
    "y_train, x_train, ids_train = load_csv_data(DATA_FOLDER + DATA_TRAIN, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train = x_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 250000)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create inverse log values of features which are positive in value.\n",
    "inv_log_cols = [0, 2, 5, 7, 9, 10, 13, 16, 19, 21, 23, 26]\n",
    "\n",
    "tx_train_inv_log_cols = np.log(1 / (1 + tx_train[inv_log_cols]))\n",
    "tx_train = np.vstack((tx_train, tx_train_inv_log_cols))\n",
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot correlations between features\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i in range(29):\n",
    "    for j in range(i+1, 30):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        x = tx_train[:,tx_train[22] == 1][i]\n",
    "        y = tx_train[:,tx_train[22] == 1][j]\n",
    "        plt.xlabel('Feature %d' % i)\n",
    "        plt.ylabel('Feature %d' % j)\n",
    "        ax.scatter(x, y)\n",
    "        plt.show()\n",
    "\n",
    "'''\n",
    "for j in range(0, 30):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    x = tx_train[22]\n",
    "    y = tx_train[j]\n",
    "    plt.xlabel('Feature %d' % 22)\n",
    "    plt.ylabel('Feature %d' % j)\n",
    "    ax.scatter(x, y)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 171334)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the data set to have the same number of 1 that -1 (downsample)\n",
    "'''\n",
    "tx_train, y_train = balance(tx_train.T, y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate into 4 sets depending on the feature 22: 'PRI_jet_num' (categorical feature)\n",
    "masks_jet_train = get_jet_masks(tx_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace remaining NaN values with the median of the column + standardie the values of all features\n",
    "tx_train = replace_nan_by_median(tx_train)\n",
    "mean_train, std_train, tx_train = standardize(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST SET ###\n",
    "\n",
    "# Do the same we did to the training set, but to the test set\n",
    "y_test, x_test, ids_test = load_csv_data(DATA_FOLDER + DATA_TEST, LOWER_BOUND, UPPER_BOUND)\n",
    "masks_jet_test = get_jet_masks(x_test)\n",
    "tx_test = x_test.T\n",
    "\n",
    "tx_test_inv_log_cols = np.log(1 / (1 + tx_test[inv_log_cols]))\n",
    "tx_test = np.vstack((tx_test, tx_test_inv_log_cols))\n",
    "\n",
    "tx_test = replace_nan_by_median(tx_test)\n",
    "tx_test = standardize_predef(tx_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 177457)\n",
      "(177457, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cross validate the different models one by one (change manually)\n",
    "model_nb = 0\n",
    "\n",
    "current_tx_train = tx_train.T[masks_jet_train[model_nb]].T\n",
    "current_y_train = y_train[masks_jet_train[model_nb]]\n",
    "\n",
    "### Those should be done if we choose not to do them before separating the data set\n",
    "'''\n",
    "# Remove columns full of NaN\n",
    "current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "\n",
    "# Remove columns without standard deviation at all\n",
    "current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "\n",
    "# Balance the data set\n",
    "#current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "# Replace remaining NaN by median\n",
    "current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "\n",
    "# Standardize features\n",
    "mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "'''\n",
    "print(current_tx_train.shape)\n",
    "print(current_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: (3, 50, 64, 0, 1.0000000000000001e-05, 0.80325162017469709)\n",
      "Finished: (3, 50, 64, 0, 2.1544346900318823e-05, 0.80308255846717369)\n",
      "Finished: (3, 50, 64, 0, 4.6415888336127818e-05, 0.80293040293040296)\n",
      "Finished: (3, 50, 64, 0, 0.0001, 0.80276697661313035)\n",
      "Finished: (3, 50, 64, 0, 0.00021544346900318823, 0.80267681036911809)\n",
      "Finished: (3, 50, 64, 0, 0.00046415888336127773, 0.80255283178360093)\n",
      "Finished: (3, 50, 64, 0, 0.001, 0.80249647788109324)\n",
      "Finished: (3, 50, 64, 0, 0.0021544346900318821, 0.80242885319808399)\n",
      "Finished: (3, 50, 64, 0, 0.0046415888336127772, 0.80250211327134413)\n",
      "Finished: (3, 50, 64, 0, 0.01, 0.8022597914905607)\n",
      "Finished: (4, 50, 64, 0, 1.0000000000000001e-05, 0.81014933784164556)\n",
      "Finished: (4, 50, 64, 0, 2.1544346900318823e-05, 0.81007607776838542)\n",
      "Finished: (4, 50, 64, 0, 4.6415888336127818e-05, 0.81013806706114411)\n",
      "Finished: (4, 50, 64, 0, 0.0001, 0.80989574528036068)\n",
      "Finished: (4, 50, 64, 0, 0.00021544346900318823, 0.80972104818258672)\n",
      "Finished: (4, 50, 64, 0, 0.00046415888336127773, 0.80921386306001697)\n",
      "Finished: (4, 50, 64, 0, 0.001, 0.80869540715694566)\n",
      "Finished: (4, 50, 64, 0, 0.0021544346900318821, 0.80743307974077216)\n",
      "Finished: (4, 50, 64, 0, 0.0046415888336127772, 0.80633981403212174)\n",
      "Finished: (4, 50, 64, 0, 0.01, 0.80538179768948992)\n",
      "Finished: (5, 50, 64, 0, 1.0000000000000001e-05, 0.81515919977458451)\n",
      "Finished: (5, 50, 64, 0, 2.1544346900318823e-05, 0.81443223443223434)\n",
      "Finished: (5, 50, 64, 0, 4.6415888336127818e-05, 0.81383488306565233)\n",
      "Finished: (5, 50, 64, 0, 0.0001, 0.81344040574809795)\n",
      "Finished: (5, 50, 64, 0, 0.00021544346900318823, 0.81315863623555928)\n",
      "Finished: (5, 50, 64, 0, 0.00046415888336127773, 0.81297830374753466)\n",
      "Finished: (5, 50, 64, 0, 0.001, 0.81273598196675112)\n",
      "Finished: (5, 50, 64, 0, 0.0021544346900318821, 0.81245984784446323)\n",
      "Finished: (5, 50, 64, 0, 0.0046415888336127772, 0.81221189067342914)\n",
      "Finished: (5, 50, 64, 0, 0.01, 0.81175542406311629)\n",
      "Finished: (6, 50, 64, 0, 1.0000000000000001e-05, 0.81777402085094386)\n",
      "Finished: (6, 50, 64, 0, 2.1544346900318823e-05, 0.81772330233868706)\n",
      "Finished: (6, 50, 64, 0, 4.6415888336127818e-05, 0.81770076077768383)\n",
      "Finished: (6, 50, 64, 0, 0.0001, 0.8175993237531699)\n",
      "Finished: (6, 50, 64, 0, 0.00021544346900318823, 0.81745280360664974)\n",
      "Finished: (6, 50, 64, 0, 0.00046415888336127773, 0.81737954353338971)\n",
      "Finished: (6, 50, 64, 0, 0.001, 0.81741335587489439)\n",
      "Finished: (6, 50, 64, 0, 0.0021544346900318821, 0.81718230487461252)\n",
      "Finished: (6, 50, 64, 0, 0.0046415888336127772, 0.81661876584953519)\n",
      "Finished: (6, 50, 64, 0, 0.01, 0.81579036348267131)\n",
      "Finished: (7, 50, 64, 0, 1.0000000000000001e-05, 0.81921668075514231)\n",
      "Finished: (7, 50, 64, 0, 2.1544346900318823e-05, 0.81151873767258387)\n",
      "Finished: (7, 50, 64, 0, 4.6415888336127818e-05, 0.81920540997464075)\n",
      "Finished: (7, 50, 64, 0, 0.0001, 0.81847280924204002)\n",
      "Finished: (7, 50, 64, 0, 0.00021544346900318823, 0.81824739363200893)\n",
      "Finished: (7, 50, 64, 0, 0.00046415888336127773, 0.79840518455903076)\n",
      "Finished: (7, 50, 64, 0, 0.001, 0.81780219780219787)\n",
      "Finished: (7, 50, 64, 0, 0.0021544346900318821, 0.81797689489997172)\n",
      "Finished: (7, 50, 64, 0, 0.0046415888336127772, 0.81805015497323197)\n",
      "Finished: (7, 50, 64, 0, 0.01, 0.81732882502113269)\n",
      "Finished: (8, 50, 64, 0, 1.0000000000000001e-05, 0.8202085094392787)\n",
      "Finished: (8, 50, 64, 0, 2.1544346900318823e-05, 0.82019160326852636)\n",
      "Finished: (8, 50, 64, 0, 4.6415888336127818e-05, 0.82005635390250775)\n",
      "Finished: (8, 50, 64, 0, 0.0001, 0.82005635390250775)\n",
      "Finished: (8, 50, 64, 0, 0.00021544346900318823, 0.82003944773175541)\n",
      "Finished: (8, 50, 64, 0, 0.00046415888336127773, 0.82001690617075229)\n",
      "Finished: (8, 50, 64, 0, 0.001, 0.81679346294730915)\n",
      "Finished: (8, 50, 64, 0, 0.0021544346900318821, 0.81978021978021975)\n",
      "Finished: (8, 50, 64, 0, 0.0046415888336127772, 0.81960552268244569)\n",
      "Finished: (8, 50, 64, 0, 0.01, 0.81926739926739922)\n",
      "Finished: (9, 50, 64, 0, 1.0000000000000001e-05, 0.82102000563539035)\n",
      "Finished: (9, 50, 64, 0, 2.1544346900318823e-05, 0.82094111017187932)\n",
      "Finished: (9, 50, 64, 0, 4.6415888336127818e-05, 0.82082840236686394)\n",
      "Finished: (9, 50, 64, 0, 0.0001, 0.82077768385460692)\n",
      "Finished: (9, 50, 64, 0, 0.00021544346900318823, 0.82074950690335302)\n",
      "Finished: (9, 50, 64, 0, 0.00046415888336127773, 0.82064806987883898)\n",
      "Finished: (9, 50, 64, 0, 0.001, 0.82081149619611171)\n",
      "Finished: (9, 50, 64, 0, 0.0021544346900318821, 0.80649196956889269)\n",
      "Finished: (9, 50, 64, 0, 0.0046415888336127772, 0.82032685263454508)\n",
      "Finished: (9, 50, 64, 0, 0.01, 0.81979149056072131)\n",
      "Finished: (10, 50, 64, 0, 1.0000000000000001e-05, 0.81134404057480969)\n",
      "Finished: (10, 50, 64, 0, 2.1544346900318823e-05, 0.7882896590588897)\n",
      "Finished: (10, 50, 64, 0, 4.6415888336127818e-05, 0.8146069315300084)\n",
      "Finished: (10, 50, 64, 0, 0.0001, 0.81647224570301502)\n",
      "Finished: (10, 50, 64, 0, 0.00021544346900318823, 0.81328825021132722)\n",
      "Finished: (10, 50, 64, 0, 0.00046415888336127773, 0.79514792899408293)\n",
      "Finished: (10, 50, 64, 0, 0.001, 0.81253874330797404)\n",
      "Finished: (10, 50, 64, 0, 0.0021544346900318821, 0.82019723865877714)\n",
      "Finished: (10, 50, 64, 0, 0.0046415888336127772, 0.82006198929275853)\n",
      "Finished: (10, 50, 64, 0, 0.01, 0.81952662721893488)\n",
      "Finished: (11, 50, 64, 0, 1.0000000000000001e-05, 0.81995491687799371)\n",
      "Finished: (11, 50, 64, 0, 2.1544346900318823e-05, 0.82125105663567211)\n",
      "Finished: (11, 50, 64, 0, 4.6415888336127818e-05, 0.82052409129332204)\n",
      "Finished: (11, 50, 64, 0, 0.0001, 0.81919413919413908)\n",
      "Finished: (11, 50, 64, 0, 0.00021544346900318823, 0.82102000563539013)\n",
      "Finished: (11, 50, 64, 0, 0.00046415888336127773, 0.82092420400112709)\n",
      "Finished: (11, 50, 64, 0, 0.001, 0.80941673710904483)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-318-4d85d8146ade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-318-4d85d8146ade>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                             w_tr, acc = cross_validation(current_y_train, tx_train_poly.T, initial_w,\n\u001b[0;32m---> 30\u001b[0;31m                                                          int(max_iter), k_indices, k, gamma, lambda_, LOWER_BOUND, UPPER_BOUND, model, batch_size)\n\u001b[0m\u001b[1;32m     31\u001b[0m                             \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                             \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/Projects/CS-433/project1/implementations.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, initial_w, max_iter, k_indices, k, gamma, lambda_, lower_bound, upper_bound, model, batch_size)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mw_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ridge_regression\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mw_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"least_squares_GD\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mw_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/ML/Projects/CS-433/project1/implementations.py\u001b[0m in \u001b[0;36mridge_regression\u001b[0;34m(y, tx, lambda_)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m#print('D: ', D, 'N: ', N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtikhonov_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtikhonov_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from implementations import build_k_indices\n",
    "#from tqdm import tqdm_notebook\n",
    "from matplotlib.pyplot import figure, show\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def cross_validation_demo():\n",
    "    model = \"ridge_regression\"\n",
    "    seed = 3\n",
    "    k_fold = 10\n",
    "    degrees = np.arange(5, 13, 1)\n",
    "    lambdas = np.logspace(-5, -2, 10)\n",
    "    gammas = [0]#np.arange(0.05, 0.6, 0.05)\n",
    "    #initial_w = init_w(tx_train)\n",
    "    max_iters = [50]#np.logspace(2, 3, 4)\n",
    "    batch_sizes = [64]\n",
    "    k_indices = build_k_indices(current_y_train, k_fold, seed)\n",
    "    results = []\n",
    "    for degree in degrees:\n",
    "        tx_train_poly = build_poly_tx(current_tx_train, degree)\n",
    "        initial_w = init_w(tx_train_poly)\n",
    "        for max_iter in max_iters:\n",
    "            for batch_size in batch_sizes:\n",
    "                for gamma in gammas:\n",
    "                    for lambda_ in lambdas:\n",
    "                        accs = []\n",
    "                        ws = []\n",
    "                        for k in range(k_fold):\n",
    "                            w_tr, acc = cross_validation(current_y_train, tx_train_poly.T, initial_w,\n",
    "                                                         int(max_iter), k_indices, k, gamma, lambda_, LOWER_BOUND, UPPER_BOUND, model, batch_size)\n",
    "                            ws.append(w_tr)\n",
    "                            accs.append(acc)\n",
    "                        w_mean = np.mean(ws, axis=0)\n",
    "                        acc_mean = np.mean(accs)\n",
    "                        results.append((degree, max_iter, batch_size, gamma, lambda_, acc_mean, w_mean))\n",
    "\n",
    "                        print(\"Finished: \" + str((degree, max_iter, batch_size, gamma, lambda_, acc_mean)))\n",
    "\n",
    "                        '''\n",
    "                        fig = plt.subplots(1, 1, figsize=(10,5))\n",
    "                        plt.plot(range(1,k_fold+1), accs, marker=\".\", color='b', label='accuracy')\n",
    "                        plt.axhline(y=acc_mean, color='r', label='mean')\n",
    "                        ax = plt.gca()\n",
    "                        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                        plt.xlabel(\"k\")\n",
    "                        plt.ylabel(\"accuracy\")\n",
    "                        plt.title(\"k-fold accuracy for lambda=%.5f, gamma=%.2f, degree=%d\" % (lambda_, gamma, degree))\n",
    "                        plt.legend(loc=2)\n",
    "                        plt.grid(True)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01,\n",
       " 0.00031622776601683794,\n",
       " 0.001,\n",
       " 0.01,\n",
       " 1.0000000000000001e-05,\n",
       " 0.0031622776601683794,\n",
       " 0.001,\n",
       " 0.00031622776601683794,\n",
       " 0.01,\n",
       " 0.001]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort(key=lambda x: -x[5])\n",
    "[x[4] for x in results[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating into multiple models\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 0, 0, 0, 0.00046415888336127773, 0.81200248756218907)\n",
    "* For 1: Best is Ridge (11, 0, 0, 0, 0.001291549665014884, 0.79104143337066068)\n",
    "* For 2: Best is Ridge (11, 50, 64, 0, 4.6415888336127818e-05, 0.84399274987053341)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 1.0000000000000001e-05, 0.80668918918918919)\n",
    "\n",
    "With balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 0.001, 0.81080816006276968)\n",
    "* For 1: Best is Ridge (13, 50, 64, 0, 0.001, 0.79271021291952359)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83351592615134906)\n",
    "* For 3: Best is Ridge (10, 50, 64, 0, 0.001, 0.80564635958395248)\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (11, 50, 64, 0, 0.0001, 0.81401432575521648)\n",
    "* For 1: Best is Ridge (11, 50, 64, 0, 3.1622776601683795e-05, 0.79476234855545214)\n",
    "* For 2: Best is Ridge (10, 50, 64, 0, 0.00031622776601683794, 0.84767954368680321)\n",
    "* For 3: Best is Ridge (9, 50, 64, 0, 0.01, 0.80967741935483883)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 50, 64, 0, 4.6415888336127818e-05, 0.84248823941547391)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.001, 0.80530049006964144)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83388921977367492)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 2.1544346900318823e-05, 0.82996389891696754)\n",
    "* For 2-3: Best is Ridge (11, 50, 64, 0, 0.001, 0.82996967190515569)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 2.1544346900318823e-05, 0.84242818536683006)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.80531338663915408)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.83325392098471307)\n",
    "* For 3: Best is Ridge (12, 50, 64, 0, 0.0001, 0.8336642599277978)\n",
    "* For 2-3: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.83195478356768682)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (10, 50, 64, 0, 0.00021544346900318823, 0.84473025723150852)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.80794428681970598)\n",
    "* For 2: Best is Ridge (9, 50, 64, 0, 1.0000000000000001e-05, 0.83732380385149896)\n",
    "* For 3: Best is Ridge (10, 50, 64, 0, 0.001, 0.83948555956678705)\n",
    "* For 2-3: Best is Ridge (10, 50, 64, 0, 0.001, 0.83607664736696985)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (9, 50, 64, 0, 4.6415888336127818e-05, 0.84425983385046544)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.80806035594531855)\n",
    "* For 2-3: Best is Ridge (10, 50, 64, 0, 0.00046415888336127773, 0.83569065343258886)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.814832692338\n",
      "Accuracy: 0.79717842633\n",
      "Accuracy: 0.850064817215\n",
      "Accuracy: 0.817173956864\n",
      "Number of 1: 234933\n",
      "Number of -1: 333305\n"
     ]
    }
   ],
   "source": [
    "# Code to use all models on the test_set and have the final prediction in y_pred\n",
    "\n",
    "degrees = [10, 12, 9, 10]\n",
    "lambdas = [0.00021544346900318823, 0.00046415888336127773, 1.0000000000000001e-05, 0.001]\n",
    "\n",
    "# Final prediction in here\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "for i in range(len(masks_jet_train)):\n",
    "    current_tx_train = tx_train.T[masks_jet_train[i]].T\n",
    "    current_tx_test = tx_test.T[masks_jet_test[i]].T\n",
    "    current_y_train = y_train[masks_jet_train[i]]\n",
    "    \n",
    "    # Preprocess here if not preprocessed before separating\n",
    "    '''\n",
    "    # Remove columns full of NaN\n",
    "    current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "    current_tx_test = current_tx_test[~np.all(np.isnan(current_tx_test), axis=1)]\n",
    "\n",
    "    # Remove columns without standard deviation at all\n",
    "    current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "    current_tx_test = current_tx_test[np.nanstd(current_tx_test, axis=1) != 0]\n",
    "    \n",
    "    # Balance the data set\n",
    "    #current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "    # Replace remaining NaN by median\n",
    "    current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "    current_tx_test = replace_nan_by_median(current_tx_test)\n",
    "\n",
    "    # Standardize features\n",
    "    mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "    current_tx_test = standardize_predef(current_tx_test, mean_train, std_train)\n",
    "    '''\n",
    "    # Build poly\n",
    "    current_tx_poly_train = build_poly_tx(current_tx_train, degrees[i])\n",
    "    current_tx_poly_test = build_poly_tx(current_tx_test, degrees[i])\n",
    "    \n",
    "    # Compute best method\n",
    "    current_w, current_loss = ridge_regression(current_y_train, current_tx_poly_train, lambdas[i])\n",
    "    \n",
    "    acc = accuracy(current_y_train, current_tx_poly_train.T, current_w, LOWER_BOUND, UPPER_BOUND)\n",
    "    \n",
    "    print(\"Accuracy:\", acc)\n",
    "    \n",
    "    # Predict\n",
    "    y_test_pred = predict_labels_kaggle(current_w, current_tx_poly_test.T, LOWER_BOUND, UPPER_BOUND)\n",
    "    y_pred[masks_jet_test[i]] = y_test_pred.flatten()\n",
    "\n",
    "print(\"Number of %d:\" % UPPER_BOUND, np.count_nonzero(y_pred == UPPER_BOUND))\n",
    "print(\"Number of %d:\" % LOWER_BOUND, np.count_nonzero(y_pred == LOWER_BOUND))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"test25.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test n : algorithm / features / y / w\n",
    "- - - - - - - - - - - - - - - - - - - \n",
    "Test 1 : least_squares / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 2 : least_squares / corr > 0.1 features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 3 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 4 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w / poly, degree=1\n",
    "\n",
    "Test 5 : least_squares / all features standardized / y = -1,1 / random init_w / median + categorical\n",
    "\n",
    "Test 6 : logistic_regressoin  /all features standardized / y = 0,1 / random init_w / median + categorical + balanced\n",
    "\n",
    "Test 7 : least_squares / all features standardized / y = 0,1 / random init_w\n",
    "\n",
    "Test 8 : Test 1\n",
    "\n",
    "Test 9 : Test 1\n",
    "\n",
    "Test 10 : Test 1 / standardized test_set with mean and std from train_set\n",
    "\n",
    "Test 11 : Test 1 / standardized test_set with mean and std from train_set / balance\n",
    "\n",
    "Test 12 : Ridge regression / non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 13 : Ridge regression / balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 14 : Ridge regression / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 15 : Ridge regression / Removed all rows containing at least a NaN / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11 / Replaced NaN values in test_set by median in test_set\n",
    "\n",
    "Test 16 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Ensembling with: (\"least_squares_GD\", 1, 150, 0, 0.01, 0), (\"least_squares_GD\", 1, 50, 0, 0.25, 0), (\"least_squares_SGD\", 1, 30, 256, 0.2, 0), (\"least_squares_SGD\", 1, 60, 64, 0.1, 0), (\"ridge_regression\", 7, 0, 0, 0, 0.001), (\"ridge_regression\", 9, 0, 0, 0, 0.001), (\"ridge_regression\", 11, 0, 0, 0, 0.001)\n",
    "\n",
    "Test 17 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 11, 11, 11] and lambdas = [0.00046415888336127773, 0.001291549665014884, 4.6415888336127818e-05, 1.0000000000000001e-05]\n",
    "\n",
    "Test 18 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 12, 12, 11] and lambdas = [4.6415888336127818e-05, 0.001, 0.0021544346900318821, 2.1544346900318823e-05]\n",
    "\n",
    "Test 19 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 12, 12, 11] and lambdas = [2.1544346900318823e-05, 0.0021544346900318821, 0.00046415888336127773, 0.0001]\n",
    "\n",
    "Test 20 : balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 13, 12, 10] and lambdas = [0.001, 0.001, 0.0021544346900318821, 0.001]\n",
    "\n",
    "Test 21 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [8, 12, 12] and lambdas = [2.1544346900318823e-05, 0.0021544346900318821, 0.00046415888336127773]\n",
    "\n",
    "Test 22 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [9, 12, 10] and lambdas = [4.6415888336127818e-05, 0.0021544346900318821, 0.00046415888336127773] / New features being inverse log\n",
    "\n",
    "Test 23 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [10, 12, 10] and lambdas = [0.00021544346900318823, 0.00046415888336127773, 0.001] / New features being inverse log\n",
    "\n",
    "Test 24 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [10, 12, 9, 10] and lambdas = [0.00021544346900318823, 0.00046415888336127773, 1.0000000000000001e-05, 0.001] / New features being inverse log\n",
    "\n",
    "Test 25 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [11, 11, 10, 9] and lambdas = [0.0001, 3.1622776601683795e-05, 0.00031622776601683794, 0.01] / New features being inverse log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- balance output (batch numpy)\n",
    "- median and category\n",
    "- features engineering : features d'intéraction\n",
    "- logistic regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
