{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from implementations import *\n",
    "from proj1_helpers import *\n",
    "from features_eng import *\n",
    "from plots import *\n",
    "from helpers import *\n",
    "from preprocessing import *\n",
    "from cross_validation import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOWER_BOUND = -1\n",
    "UPPER_BOUND = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"competition-data/\"\n",
    "DATA_TEST = \"test.csv\"\n",
    "DATA_TRAIN = \"train.csv\"\n",
    "y_train, x_train, ids_train = load_csv_data(DATA_FOLDER + DATA_TRAIN, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train = x_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 250000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create inverse log values of features which are positive in value.\n",
    "inv_log_cols = [0, 2, 5, 7, 9, 10, 13, 16, 19, 21, 23, 26]\n",
    "\n",
    "tx_train_inv_log_cols = np.log(1 / (1 + tx_train[inv_log_cols]))\n",
    "tx_train = np.vstack((tx_train, tx_train_inv_log_cols))\n",
    "tx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor j in range(0, 30):\\n    fig = plt.figure()\\n    ax = fig.add_subplot(1, 1, 1)\\n    x = tx_train[22]\\n    y = tx_train[j]\\n    plt.xlabel('Feature %d' % 22)\\n    plt.ylabel('Feature %d' % j)\\n    ax.scatter(x, y)\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to plot correlations between features\n",
    "\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i in range(29):\n",
    "    for j in range(i+1, 30):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        x = tx_train[:,tx_train[22] == 1][i]\n",
    "        y = tx_train[:,tx_train[22] == 1][j]\n",
    "        plt.xlabel('Feature %d' % i)\n",
    "        plt.ylabel('Feature %d' % j)\n",
    "        ax.scatter(x, y)\n",
    "        plt.show()\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "for j in range(0, 30):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    x = tx_train[22]\n",
    "    y = tx_train[j]\n",
    "    plt.xlabel('Feature %d' % 22)\n",
    "    plt.ylabel('Feature %d' % j)\n",
    "    ax.scatter(x, y)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntx_train, y_train = balance(tx_train.T, y_train, LOWER_BOUND, UPPER_BOUND)\\ntx_train.shape\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance the data set to have the same number of 1 that -1 (downsample)\n",
    "'''\n",
    "tx_train, y_train = balance(tx_train.T, y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "tx_train.shape\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into 4 sets depending on the feature 22: 'PRI_jet_num' (categorical feature)\n",
    "masks_jet_train = get_jet_masks(tx_train.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace remaining NaN values with the median of the column + standardie the values of all features\n",
    "tx_train = replace_nan_by_median(tx_train)\n",
    "mean_train, std_train, tx_train = standardize(tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TEST SET ###\n",
    "\n",
    "# Do the same we did to the training set, but to the test set\n",
    "y_test, x_test, ids_test = load_csv_data(DATA_FOLDER + DATA_TEST, LOWER_BOUND, UPPER_BOUND)\n",
    "masks_jet_test = get_jet_masks(x_test)\n",
    "tx_test = x_test.T\n",
    "\n",
    "tx_test_inv_log_cols = np.log(1 / (1 + tx_test[inv_log_cols]))\n",
    "tx_test = np.vstack((tx_test, tx_test_inv_log_cols))\n",
    "\n",
    "tx_test = replace_nan_by_median(tx_test)\n",
    "tx_test = standardize_predef(tx_test, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 99913)\n",
      "(99913, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cross validate the different models one by one (change manually)\n",
    "model_nb = 0\n",
    "\n",
    "current_tx_train = tx_train.T[masks_jet_train[model_nb]].T\n",
    "current_y_train = y_train[masks_jet_train[model_nb]]\n",
    "\n",
    "### Those should be done if we choose not to do them before separating the data set\n",
    "'''\n",
    "# Remove columns full of NaN\n",
    "current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "\n",
    "# Remove columns without standard deviation at all\n",
    "current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "\n",
    "# Balance the data set\n",
    "#current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "# Replace remaining NaN by median\n",
    "current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "\n",
    "# Standardize features\n",
    "mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "'''\n",
    "print(current_tx_train.shape)\n",
    "print(current_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f475f4cdb94eff91024945105820a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: (1, 50, 64, 0, 1.0000000000000001e-05, 0.82763487138424596)\n",
      "Finished: (1, 50, 64, 0, 1.6237767391887208e-05, 0.82758482634370945)\n",
      "Finished: (1, 50, 64, 0, 2.6366508987303556e-05, 0.82762486237613864)\n",
      "Finished: (1, 50, 64, 0, 4.2813323987193957e-05, 0.82766489840856783)\n",
      "Finished: (1, 50, 64, 0, 6.9519279617756056e-05, 0.82757481733560212)\n",
      "Finished: (1, 50, 64, 0, 0.00011288378916846884, 0.82753478130317293)\n",
      "Finished: (1, 50, 64, 0, 0.00018329807108324357, 0.82742468221399279)\n",
      "Finished: (1, 50, 64, 0, 0.00029763514416313193, 0.82737463717345605)\n",
      "Finished: (1, 50, 64, 0, 0.00048329302385717522, 0.82735461915724162)\n",
      "Finished: (1, 50, 64, 0, 0.00078475997035146064, 0.82725452907616859)\n",
      "Finished: (1, 50, 64, 0, 0.0012742749857031334, 0.82716444800320288)\n",
      "Finished: (1, 50, 64, 0, 0.0020691380811147901, 0.82694424982484238)\n",
      "Finished: (1, 50, 64, 0, 0.0033598182862837811, 0.8269142228005204)\n",
      "Finished: (1, 50, 64, 0, 0.0054555947811685147, 0.82685416875187667)\n",
      "Finished: (1, 50, 64, 0, 0.0088586679041008226, 0.82641377239515568)\n",
      "Finished: (1, 50, 64, 0, 0.01438449888287663, 0.82591332198979084)\n",
      "Finished: (1, 50, 64, 0, 0.023357214690901212, 0.824972475227705)\n",
      "Finished: (1, 50, 64, 0, 0.037926901907322459, 0.82347112401161038)\n",
      "Finished: (1, 50, 64, 0, 0.061584821106602607, 0.82169952957661896)\n",
      "Finished: (1, 50, 64, 0, 0.10000000000000001, 0.81860674607146433)\n",
      "Finished: (2, 50, 64, 0, 1.0000000000000001e-05, 0.83392052847562803)\n",
      "Finished: (2, 50, 64, 0, 1.6237767391887208e-05, 0.83395055549995001)\n",
      "Finished: (2, 50, 64, 0, 2.6366508987303556e-05, 0.83375037533780405)\n",
      "Finished: (2, 50, 64, 0, 4.2813323987193957e-05, 0.83359023120808717)\n",
      "Finished: (2, 50, 64, 0, 6.9519279617756056e-05, 0.83330997898108294)\n",
      "Finished: (2, 50, 64, 0, 0.00011288378916846884, 0.83299969972975685)\n",
      "Finished: (2, 50, 64, 0, 0.00018329807108324357, 0.83243919527574817)\n",
      "Finished: (2, 50, 64, 0, 0.00029763514416313193, 0.83208887999199277)\n",
      "Finished: (2, 50, 64, 0, 0.00048329302385717522, 0.83151836652987687)\n",
      "Finished: (2, 50, 64, 0, 0.00078475997035146064, 0.83130817735962359)\n",
      "Finished: (2, 50, 64, 0, 0.0012742749857031334, 0.83095786207586819)\n",
      "Finished: (2, 50, 64, 0, 0.0020691380811147901, 0.83043739365428881)\n",
      "Finished: (2, 50, 64, 0, 0.0033598182862837811, 0.83014713241917737)\n",
      "Finished: (2, 50, 64, 0, 0.0054555947811685147, 0.82984686217595838)\n",
      "Finished: (2, 50, 64, 0, 0.0088586679041008226, 0.82984686217595838)\n",
      "Finished: (2, 50, 64, 0, 0.01438449888287663, 0.82984686217595838)\n",
      "Finished: (2, 50, 64, 0, 0.023357214690901212, 0.82953658292463217)\n",
      "Finished: (2, 50, 64, 0, 0.037926901907322459, 0.82933640276248632)\n",
      "Finished: (2, 50, 64, 0, 0.061584821106602607, 0.8285757181463318)\n",
      "Finished: (2, 50, 64, 0, 0.10000000000000001, 0.82792513261935741)\n",
      "Finished: (3, 50, 64, 0, 1.0000000000000001e-05, 0.83633269942948663)\n",
      "Finished: (3, 50, 64, 0, 1.6237767391887208e-05, 0.83618256430787707)\n",
      "Finished: (3, 50, 64, 0, 2.6366508987303556e-05, 0.83595235712140925)\n",
      "Finished: (3, 50, 64, 0, 4.2813323987193957e-05, 0.83591232108898017)\n",
      "Finished: (3, 50, 64, 0, 6.9519279617756056e-05, 0.83567210489440491)\n",
      "Finished: (3, 50, 64, 0, 0.00011288378916846884, 0.83563206886197572)\n",
      "Finished: (3, 50, 64, 0, 0.00018329807108324357, 0.83563206886197572)\n",
      "Finished: (3, 50, 64, 0, 0.00029763514416313193, 0.83552196977279558)\n",
      "Finished: (3, 50, 64, 0, 0.00048329302385717522, 0.83565208687819026)\n",
      "Finished: (3, 50, 64, 0, 0.00078475997035146064, 0.83577219497547794)\n",
      "Finished: (3, 50, 64, 0, 0.0012742749857031334, 0.83562205985386839)\n",
      "Finished: (3, 50, 64, 0, 0.0020691380811147901, 0.83539185266740079)\n",
      "Finished: (3, 50, 64, 0, 0.0033598182862837811, 0.83515163647282553)\n",
      "Finished: (3, 50, 64, 0, 0.0054555947811685147, 0.83488139325392863)\n",
      "Finished: (3, 50, 64, 0, 0.0088586679041008226, 0.83477129416474827)\n",
      "Finished: (3, 50, 64, 0, 0.01438449888287663, 0.83422079871884702)\n",
      "Finished: (3, 50, 64, 0, 0.023357214690901212, 0.83363026724051648)\n",
      "Finished: (3, 50, 64, 0, 0.037926901907322459, 0.83263937543789412)\n",
      "Finished: (3, 50, 64, 0, 0.061584821106602607, 0.83174857371634481)\n",
      "Finished: (3, 50, 64, 0, 0.10000000000000001, 0.83034731258132322)\n",
      "Finished: (4, 50, 64, 0, 1.0000000000000001e-05, 0.83883495145631071)\n",
      "Finished: (4, 50, 64, 0, 1.6237767391887208e-05, 0.83881493344009628)\n",
      "Finished: (4, 50, 64, 0, 2.6366508987303556e-05, 0.83884496046441814)\n",
      "Finished: (4, 50, 64, 0, 4.2813323987193957e-05, 0.83877489740766698)\n",
      "Finished: (4, 50, 64, 0, 6.9519279617756056e-05, 0.83868481633470127)\n",
      "Finished: (4, 50, 64, 0, 0.00011288378916846884, 0.83852467220498439)\n",
      "Finished: (4, 50, 64, 0, 0.00018329807108324357, 0.83828445601040935)\n",
      "Finished: (4, 50, 64, 0, 0.00029763514416313193, 0.8380442398158342)\n",
      "Finished: (4, 50, 64, 0, 0.00048329302385717522, 0.83776398758883008)\n",
      "Finished: (4, 50, 64, 0, 0.00078475997035146064, 0.8374236813131819)\n",
      "Finished: (4, 50, 64, 0, 0.0012742749857031334, 0.83728355519967967)\n",
      "Finished: (4, 50, 64, 0, 0.0020691380811147901, 0.83686317685917333)\n",
      "Finished: (4, 50, 64, 0, 0.0033598182862837811, 0.83664297868081261)\n",
      "Finished: (4, 50, 64, 0, 0.0054555947811685147, 0.83618256430787707)\n",
      "Finished: (4, 50, 64, 0, 0.0088586679041008226, 0.83576218596737062)\n",
      "Finished: (4, 50, 64, 0, 0.01438449888287663, 0.83528175357822043)\n",
      "Finished: (4, 50, 64, 0, 0.023357214690901212, 0.83477129416474827)\n",
      "Finished: (4, 50, 64, 0, 0.037926901907322459, 0.83417075367831051)\n",
      "Finished: (4, 50, 64, 0, 0.061584821106602607, 0.83303973576218593)\n",
      "Finished: (4, 50, 64, 0, 0.10000000000000001, 0.83198878991091996)\n",
      "Finished: (5, 50, 64, 0, 1.0000000000000001e-05, 0.84080672605344797)\n",
      "Finished: (5, 50, 64, 0, 1.6237767391887208e-05, 0.84060654589130213)\n",
      "Finished: (5, 50, 64, 0, 2.6366508987303556e-05, 0.84049644680212199)\n",
      "Finished: (5, 50, 64, 0, 4.2813323987193957e-05, 0.84041637473726349)\n",
      "Finished: (5, 50, 64, 0, 6.9519279617756056e-05, 0.84029626663997592)\n",
      "Finished: (5, 50, 64, 0, 0.00011288378916846884, 0.840196176558903)\n",
      "Finished: (5, 50, 64, 0, 0.00018329807108324357, 0.84016614953458113)\n",
      "Finished: (5, 50, 64, 0, 0.00029763514416313193, 0.84012611350215194)\n",
      "Finished: (5, 50, 64, 0, 0.00048329302385717522, 0.8401561405264738)\n",
      "Finished: (5, 50, 64, 0, 0.00078475997035146064, 0.84016614953458113)\n",
      "Finished: (5, 50, 64, 0, 0.0012742749857031334, 0.84020618556701032)\n",
      "Finished: (5, 50, 64, 0, 0.0020691380811147901, 0.84014613151836648)\n",
      "Finished: (5, 50, 64, 0, 0.0033598182862837811, 0.83973576218596746)\n",
      "Finished: (5, 50, 64, 0, 0.0054555947811685147, 0.83933540186167543)\n",
      "Finished: (5, 50, 64, 0, 0.0088586679041008226, 0.83885496947252525)\n",
      "Finished: (5, 50, 64, 0, 0.01438449888287663, 0.83858472625362823)\n",
      "Finished: (5, 50, 64, 0, 0.023357214690901212, 0.83809428485637072)\n",
      "Finished: (5, 50, 64, 0, 0.037926901907322459, 0.83776398758882986)\n",
      "Finished: (5, 50, 64, 0, 0.061584821106602607, 0.83630267240516465)\n",
      "Finished: (5, 50, 64, 0, 0.10000000000000001, 0.83452106896206568)\n",
      "Finished: (6, 50, 64, 0, 1.0000000000000001e-05, 0.84211790611550386)\n",
      "Finished: (6, 50, 64, 0, 1.6237767391887208e-05, 0.84184766289660706)\n",
      "Finished: (6, 50, 64, 0, 2.6366508987303556e-05, 0.84235812231007912)\n",
      "Finished: (6, 50, 64, 0, 4.2813323987193957e-05, 0.84223801421279154)\n",
      "Finished: (6, 50, 64, 0, 6.9519279617756056e-05, 0.84129716745070571)\n",
      "Finished: (6, 50, 64, 0, 0.00011288378916846884, 0.83649284355920328)\n",
      "Finished: (6, 50, 64, 0, 0.00018329807108324357, 0.84227805024522051)\n",
      "Finished: (6, 50, 64, 0, 0.00029763514416313193, 0.8423981583425082)\n",
      "Finished: (6, 50, 64, 0, 0.00048329302385717522, 0.82285056550895808)\n",
      "Finished: (6, 50, 64, 0, 0.00078475997035146064, 0.84120708637774)\n",
      "Finished: (6, 50, 64, 0, 0.0012742749857031334, 0.8417375638074267)\n",
      "Finished: (6, 50, 64, 0, 0.0020691380811147901, 0.83647282554298863)\n",
      "Finished: (6, 50, 64, 0, 0.0033598182862837811, 0.84174757281553403)\n",
      "Finished: (6, 50, 64, 0, 0.0054555947811685147, 0.84127714943449095)\n",
      "Finished: (6, 50, 64, 0, 0.0088586679041008226, 0.84028625763186859)\n",
      "Finished: (6, 50, 64, 0, 0.01438449888287663, 0.84025623060754684)\n",
      "Finished: (6, 50, 64, 0, 0.023357214690901212, 0.83997597838054239)\n",
      "Finished: (6, 50, 64, 0, 0.037926901907322459, 0.83853468121309171)\n",
      "Finished: (6, 50, 64, 0, 0.061584821106602607, 0.83760384345911321)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: (6, 50, 64, 0, 0.10000000000000001, 0.83552196977279558)\n",
      "Finished: (7, 50, 64, 0, 1.0000000000000001e-05, 0.84335902312080857)\n",
      "Finished: (7, 50, 64, 0, 1.6237767391887208e-05, 0.84338905014513055)\n",
      "Finished: (7, 50, 64, 0, 2.6366508987303556e-05, 0.84339905915323787)\n",
      "Finished: (7, 50, 64, 0, 4.2813323987193957e-05, 0.84325893303973576)\n",
      "Finished: (7, 50, 64, 0, 6.9519279617756056e-05, 0.84328896006405762)\n",
      "Finished: (7, 50, 64, 0, 0.00011288378916846884, 0.8430987889100191)\n",
      "Finished: (7, 50, 64, 0, 0.00018329807108324357, 0.84221799619657689)\n",
      "Finished: (7, 50, 64, 0, 0.00029763514416313193, 0.83181863677309575)\n",
      "Finished: (7, 50, 64, 0, 0.00048329302385717522, 0.84250825743168856)\n",
      "Finished: (7, 50, 64, 0, 0.00078475997035146064, 0.84293864478030223)\n",
      "Finished: (7, 50, 64, 0, 0.0012742749857031334, 0.83633269942948663)\n",
      "Finished: (7, 50, 64, 0, 0.0020691380811147901, 0.84253828445601031)\n",
      "Finished: (7, 50, 64, 0, 0.0033598182862837811, 0.84288859973976571)\n",
      "Finished: (7, 50, 64, 0, 0.0054555947811685147, 0.84177759983985589)\n",
      "Finished: (7, 50, 64, 0, 0.0088586679041008226, 0.84124712241016919)\n",
      "Finished: (7, 50, 64, 0, 0.01438449888287663, 0.83880492443198873)\n",
      "Finished: (7, 50, 64, 0, 0.023357214690901212, 0.84028625763186859)\n",
      "Finished: (7, 50, 64, 0, 0.037926901907322459, 0.83889500550495444)\n",
      "Finished: (7, 50, 64, 0, 0.061584821106602607, 0.83752377139425482)\n",
      "Finished: (7, 50, 64, 0, 0.10000000000000001, 0.83541187068361522)\n",
      "Finished: (8, 50, 64, 0, 1.0000000000000001e-05, 0.84334901411270147)\n",
      "Finished: (8, 50, 64, 0, 1.6237767391887208e-05, 0.84392953658292469)\n",
      "Finished: (8, 50, 64, 0, 2.6366508987303556e-05, 0.84167750975878286)\n",
      "Finished: (8, 50, 64, 0, 4.2813323987193957e-05, 0.84422980682614368)\n",
      "Finished: (8, 50, 64, 0, 6.9519279617756056e-05, 0.84361925733159848)\n",
      "Finished: (8, 50, 64, 0, 0.00011288378916846884, 0.84055650085076583)\n",
      "Finished: (8, 50, 64, 0, 0.00018329807108324357, 0.84382944650185165)\n",
      "Finished: (8, 50, 64, 0, 0.00029763514416313193, 0.84124712241016919)\n",
      "Finished: (8, 50, 64, 0, 0.00048329302385717522, 0.84376939245320792)\n",
      "Finished: (8, 50, 64, 0, 0.00078475997035146064, 0.84322890601541389)\n",
      "Finished: (8, 50, 64, 0, 0.0012742749857031334, 0.84362926633970581)\n",
      "Finished: (8, 50, 64, 0, 0.0020691380811147901, 0.84282854569112187)\n",
      "Finished: (8, 50, 64, 0, 0.0033598182862837811, 0.84282854569112187)\n",
      "Finished: (8, 50, 64, 0, 0.0054555947811685147, 0.84240816735061552)\n",
      "Finished: (8, 50, 64, 0, 0.0088586679041008226, 0.8421779601641477)\n",
      "Finished: (8, 50, 64, 0, 0.01438449888287663, 0.84151736562906598)\n",
      "Finished: (8, 50, 64, 0, 0.023357214690901212, 0.84079671704534087)\n",
      "Finished: (8, 50, 64, 0, 0.037926901907322459, 0.83961565408867977)\n",
      "Finished: (8, 50, 64, 0, 0.061584821106602607, 0.83793414072665406)\n",
      "Finished: (8, 50, 64, 0, 0.10000000000000001, 0.83556200580522477)\n",
      "Finished: (9, 50, 64, 0, 1.0000000000000001e-05, 0.84422980682614346)\n",
      "Finished: (9, 50, 64, 0, 1.6237767391887208e-05, 0.84445000500450396)\n",
      "Finished: (9, 50, 64, 0, 2.6366508987303556e-05, 0.84410969872885599)\n",
      "Finished: (9, 50, 64, 0, 4.2813323987193957e-05, 0.841787608847963)\n",
      "Finished: (9, 50, 64, 0, 6.9519279617756056e-05, 0.83442097888099287)\n",
      "Finished: (9, 50, 64, 0, 0.00011288378916846884, 0.81272144930437396)\n",
      "Finished: (9, 50, 64, 0, 0.00018329807108324357, 0.84441997798018209)\n",
      "Finished: (9, 50, 64, 0, 0.00029763514416313193, 0.84356921229106197)\n",
      "Finished: (9, 50, 64, 0, 0.00048329302385717522, 0.84408968071264145)\n",
      "Finished: (9, 50, 64, 0, 0.00078475997035146064, 0.84415974376939251)\n",
      "Finished: (9, 50, 64, 0, 0.0012742749857031334, 0.84383945550995898)\n",
      "Finished: (9, 50, 64, 0, 0.0020691380811147901, 0.8435391852667401)\n",
      "Finished: (9, 50, 64, 0, 0.0033598182862837811, 0.84385947352617363)\n",
      "Finished: (9, 50, 64, 0, 0.0054555947811685147, 0.84366930237213489)\n",
      "Finished: (9, 50, 64, 0, 0.0088586679041008226, 0.84305875287758991)\n",
      "Finished: (9, 50, 64, 0, 0.01438449888287663, 0.84209788809928932)\n",
      "Finished: (9, 50, 64, 0, 0.023357214690901212, 0.84126714042638362)\n",
      "Finished: (9, 50, 64, 0, 0.037926901907322459, 0.84011610449404461)\n",
      "Finished: (9, 50, 64, 0, 0.061584821106602607, 0.83816434791312167)\n",
      "Finished: (9, 50, 64, 0, 0.10000000000000001, 0.83375037533780405)\n",
      "Finished: (10, 50, 64, 0, 1.0000000000000001e-05, 0.80221199079171268)\n",
      "Finished: (10, 50, 64, 0, 1.6237767391887208e-05, 0.79979981983785409)\n",
      "Finished: (10, 50, 64, 0, 2.6366508987303556e-05, 0.8001601441297167)\n",
      "Finished: (10, 50, 64, 0, 4.2813323987193957e-05, 0.83340006005404865)\n",
      "Finished: (10, 50, 64, 0, 6.9519279617756056e-05, 0.78952056851166041)\n",
      "Finished: (10, 50, 64, 0, 0.00011288378916846884, 0.84386948253428096)\n",
      "Finished: (10, 50, 64, 0, 0.00018329807108324357, 0.84450005004504047)\n",
      "Finished: (10, 50, 64, 0, 0.00029763514416313193, 0.84479031128015214)\n",
      "Finished: (10, 50, 64, 0, 0.00048329302385717522, 0.84448003202882593)\n",
      "Finished: (10, 50, 64, 0, 0.00078475997035146064, 0.84128715844259827)\n",
      "Finished: (10, 50, 64, 0, 0.0012742749857031334, 0.84383945550995898)\n",
      "Finished: (10, 50, 64, 0, 0.0020691380811147901, 0.84439995996396744)\n",
      "Finished: (10, 50, 64, 0, 0.0033598182862837811, 0.8437593834451006)\n",
      "Finished: (10, 50, 64, 0, 0.0054555947811685147, 0.80988890001000902)\n",
      "Finished: (10, 50, 64, 0, 0.0088586679041008226, 0.84341907716945241)\n",
      "Finished: (10, 50, 64, 0, 0.01438449888287663, 0.84285857271544384)\n",
      "Finished: (10, 50, 64, 0, 0.023357214690901212, 0.84165749174256832)\n",
      "Finished: (10, 50, 64, 0, 0.037926901907322459, 0.84016614953458113)\n",
      "Finished: (10, 50, 64, 0, 0.061584821106602607, 0.83841457311580425)\n",
      "Finished: (10, 50, 64, 0, 0.10000000000000001, 0.82901611450305279)\n",
      "Finished: (11, 50, 64, 0, 1.0000000000000001e-05, 0.8258132319087178)\n",
      "Finished: (11, 50, 64, 0, 1.6237767391887208e-05, 0.79364427985186659)\n",
      "Finished: (11, 50, 64, 0, 2.6366508987303556e-05, 0.81147032329096191)\n",
      "Finished: (11, 50, 64, 0, 4.2813323987193957e-05, 0.76184566109498553)\n",
      "Finished: (11, 50, 64, 0, 6.9519279617756056e-05, 0.80219197277549803)\n",
      "Finished: (11, 50, 64, 0, 0.00011288378916846884, 0.78881993794414973)\n",
      "Finished: (11, 50, 64, 0, 0.00018329807108324357, 0.81163046742067857)\n",
      "Finished: (11, 50, 64, 0, 0.00029763514416313193, 0.83039735762185973)\n",
      "Finished: (11, 50, 64, 0, 0.00048329302385717522, 0.82696426784105692)\n",
      "Finished: (11, 50, 64, 0, 0.00078475997035146064, 0.82050845761185054)\n",
      "Finished: (11, 50, 64, 0, 0.0012742749857031334, 0.83522169952957659)\n",
      "Finished: (11, 50, 64, 0, 0.0020691380811147901, 0.84196777099389453)\n",
      "Finished: (11, 50, 64, 0, 0.0033598182862837811, 0.82336102492243024)\n",
      "Finished: (11, 50, 64, 0, 0.0054555947811685147, 0.84141727554799317)\n",
      "Finished: (11, 50, 64, 0, 0.0088586679041008226, 0.84280852767490744)\n",
      "Finished: (11, 50, 64, 0, 0.01438449888287663, 0.8425783204884395)\n",
      "Finished: (11, 50, 64, 0, 0.023357214690901212, 0.84178760884796322)\n",
      "Finished: (11, 50, 64, 0, 0.037926901907322459, 0.83986587929136236)\n",
      "Finished: (11, 50, 64, 0, 0.061584821106602607, 0.83831448303473122)\n",
      "Finished: (11, 50, 64, 0, 0.10000000000000001, 0.83664297868081261)\n",
      "Finished: (12, 50, 64, 0, 1.0000000000000001e-05, 0.72977679911920734)\n",
      "Finished: (12, 50, 64, 0, 1.6237767391887208e-05, 0.77300570513462119)\n",
      "Finished: (12, 50, 64, 0, 2.6366508987303556e-05, 0.78585727154439)\n",
      "Finished: (12, 50, 64, 0, 4.2813323987193957e-05, 0.82654388950055036)\n",
      "Finished: (12, 50, 64, 0, 6.9519279617756056e-05, 0.81258132319087184)\n",
      "Finished: (12, 50, 64, 0, 0.00011288378916846884, 0.82593334000600538)\n",
      "Finished: (12, 50, 64, 0, 0.00018329807108324357, 0.80019017115403857)\n",
      "Finished: (12, 50, 64, 0, 0.00029763514416313193, 0.80910819737763995)\n",
      "Finished: (12, 50, 64, 0, 0.00048329302385717522, 0.83395055549994979)\n",
      "Finished: (12, 50, 64, 0, 0.00078475997035146064, 0.80958862976679014)\n",
      "Finished: (12, 50, 64, 0, 0.0012742749857031334, 0.833820438394555)\n",
      "Finished: (12, 50, 64, 0, 0.0020691380811147901, 0.82770493444099702)\n",
      "Finished: (12, 50, 64, 0, 0.0033598182862837811, 0.83859473526173556)\n",
      "Finished: (12, 50, 64, 0, 0.0054555947811685147, 0.83159843859473526)\n",
      "Finished: (12, 50, 64, 0, 0.0088586679041008226, 0.84137723951556409)\n",
      "Finished: (12, 50, 64, 0, 0.01438449888287663, 0.81294164748273445)\n",
      "Finished: (12, 50, 64, 0, 0.023357214690901212, 0.83662296066459807)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: (12, 50, 64, 0, 0.037926901907322459, 0.82653388049244325)\n",
      "Finished: (12, 50, 64, 0, 0.061584821106602607, 0.83133820438394557)\n",
      "Finished: (12, 50, 64, 0, 0.10000000000000001, 0.82183965569012096)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def cross_validation_demo():\n",
    "    model = \"ridge_regression\"\n",
    "    seed = 3\n",
    "    k_fold = 10\n",
    "    degrees = np.arange(1, 13, 1)\n",
    "    lambdas = np.logspace(-5, -1, 20)\n",
    "    gammas = [0]#np.arange(0.05, 0.6, 0.05)\n",
    "    #initial_w = init_w(tx_train)\n",
    "    max_iters = [50]#np.logspace(2, 3, 4)\n",
    "    batch_sizes = [64]\n",
    "    k_indices = build_k_indices(current_y_train, k_fold, seed)\n",
    "    results = []\n",
    "    for degree in tqdm_notebook(degrees):\n",
    "        tx_train_poly = build_poly_tx(current_tx_train, degree)\n",
    "        initial_w = init_w(tx_train_poly)\n",
    "        for max_iter in max_iters:\n",
    "            for batch_size in batch_sizes:\n",
    "                for gamma in gammas:\n",
    "                    for i, lambda_ in enumerate(lambdas):\n",
    "                        accs = []\n",
    "                        ws = []\n",
    "                        for k in range(k_fold):\n",
    "                            w_tr, acc = cross_validation(current_y_train, tx_train_poly.T, initial_w,\n",
    "                                                         int(max_iter), k_indices, k, gamma, lambda_, LOWER_BOUND, UPPER_BOUND, model, batch_size)\n",
    "                            ws.append(w_tr)\n",
    "                            accs.append(acc)\n",
    "                        w_mean = np.mean(ws, axis=0)\n",
    "                        acc_mean = np.mean(accs)\n",
    "                        results.append((degree, max_iter, batch_size, gamma, lambda_, acc_mean, w_mean, i))\n",
    "\n",
    "                        print(\"Finished: \" + str((degree, max_iter, batch_size, gamma, lambda_, acc_mean)))\n",
    "\n",
    "                        '''\n",
    "                        fig = plt.subplots(1, 1, figsize=(10,5))\n",
    "                        plt.plot(range(1,k_fold+1), accs, marker=\".\", color='b', label='accuracy')\n",
    "                        plt.axhline(y=acc_mean, color='r', label='mean')\n",
    "                        ax = plt.gca()\n",
    "                        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                        plt.xlabel(\"k\")\n",
    "                        plt.ylabel(\"accuracy\")\n",
    "                        plt.title(\"k-fold accuracy for lambda=%.5f, gamma=%.2f, degree=%d\" % (lambda_, gamma, degree))\n",
    "                        plt.legend(loc=2)\n",
    "                        plt.grid(True)\n",
    "                        plt.show()\n",
    "                        '''\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAGvCAYAAABhBh1VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12771aac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "degrees = np.arange(1, 13, 1)\n",
    "lambdas = np.logspace(-5, -1, 20)\n",
    "\n",
    "show_ridge_performance(results, degrees, lambdas, '0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.sort(key=lambda x: -x[5])\n",
    "[x[4] for x in results[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating into multiple models\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 0, 0, 0, 0.00046415888336127773, 0.81200248756218907)\n",
    "* For 1: Best is Ridge (11, 0, 0, 0, 0.001291549665014884, 0.79104143337066068)\n",
    "* For 2: Best is Ridge (11, 50, 64, 0, 4.6415888336127818e-05, 0.84399274987053341)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 1.0000000000000001e-05, 0.80668918918918919)\n",
    "\n",
    "With balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 0.001, 0.81080816006276968)\n",
    "* For 1: Best is Ridge (13, 50, 64, 0, 0.001, 0.79271021291952359)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83351592615134906)\n",
    "* For 3: Best is Ridge (10, 50, 64, 0, 0.001, 0.80564635958395248)\n",
    "\n",
    "With balance before everything (+ nan_to_median + standardize) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (11, 50, 64, 0, 0.0001, 0.81401432575521648)\n",
    "* For 1: Best is Ridge (11, 50, 64, 0, 3.1622776601683795e-05, 0.79476234855545214)\n",
    "* For 2: Best is Ridge (10, 50, 64, 0, 0.00031622776601683794, 0.84767954368680321)\n",
    "* For 3: Best is Ridge (9, 50, 64, 0, 0.01, 0.80967741935483883)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize):\n",
    "\n",
    "* For 0: Best is Ridge (9, 50, 64, 0, 4.6415888336127818e-05, 0.84248823941547391)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.001, 0.80530049006964144)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.83388921977367492)\n",
    "* For 3: Best is Ridge (11, 50, 64, 0, 2.1544346900318823e-05, 0.82996389891696754)\n",
    "* For 2-3: Best is Ridge (11, 50, 64, 0, 0.001, 0.82996967190515569)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA):\n",
    "\n",
    "* For 0: Best is Ridge (8, 50, 64, 0, 2.1544346900318823e-05, 0.84242818536683006)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.80531338663915408)\n",
    "* For 2: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.83325392098471307)\n",
    "* For 3: Best is Ridge (12, 50, 64, 0, 0.0001, 0.8336642599277978)\n",
    "* For 2-3: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.83195478356768682)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (10, 50, 64, 0, 0.00021544346900318823, 0.84473025723150852)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.00046415888336127773, 0.80794428681970598)\n",
    "* For 2: Best is Ridge (9, 50, 64, 0, 1.0000000000000001e-05, 0.83732380385149896)\n",
    "* For 3: Best is Ridge (10, 50, 64, 0, 0.001, 0.83948555956678705)\n",
    "* For 2-3: Best is Ridge (10, 50, 64, 0, 0.001, 0.83607664736696985)\n",
    "\n",
    "Without balance (+ nan_to_median + standardize BUT AFTER SEPARATING DATA) and with added inv. log:\n",
    "\n",
    "* For 0: Best is Ridge (9, 50, 64, 0, 4.6415888336127818e-05, 0.84425983385046544)\n",
    "* For 1: Best is Ridge (12, 50, 64, 0, 0.0021544346900318821, 0.80806035594531855)\n",
    "* For 2-3: Best is Ridge (10, 50, 64, 0, 0.00046415888336127773, 0.83569065343258886)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Code to use all models on the test_set and have the final prediction in y_pred\n",
    "\n",
    "degrees = [10, 12, 9, 10]\n",
    "lambdas = [0.00021544346900318823, 0.00046415888336127773, 1.0000000000000001e-05, 0.001]\n",
    "\n",
    "# Final prediction in here\n",
    "y_pred = np.zeros(x_test.shape[0])\n",
    "\n",
    "for i in range(len(masks_jet_train)):\n",
    "    current_tx_train = tx_train.T[masks_jet_train[i]].T\n",
    "    current_tx_test = tx_test.T[masks_jet_test[i]].T\n",
    "    current_y_train = y_train[masks_jet_train[i]]\n",
    "    \n",
    "    # Preprocess here if not preprocessed before separating\n",
    "    '''\n",
    "    # Remove columns full of NaN\n",
    "    current_tx_train = current_tx_train[~np.all(np.isnan(current_tx_train), axis=1)]\n",
    "    current_tx_test = current_tx_test[~np.all(np.isnan(current_tx_test), axis=1)]\n",
    "\n",
    "    # Remove columns without standard deviation at all\n",
    "    current_tx_train = current_tx_train[np.nanstd(current_tx_train, axis=1) != 0]\n",
    "    current_tx_test = current_tx_test[np.nanstd(current_tx_test, axis=1) != 0]\n",
    "    \n",
    "    # Balance the data set\n",
    "    #current_tx_train, current_y_train = balance(current_tx_train.T, current_y_train, LOWER_BOUND, UPPER_BOUND)\n",
    "\n",
    "    # Replace remaining NaN by median\n",
    "    current_tx_train = replace_nan_by_median(current_tx_train)\n",
    "    current_tx_test = replace_nan_by_median(current_tx_test)\n",
    "\n",
    "    # Standardize features\n",
    "    mean_train, std_train, current_tx_train = standardize(current_tx_train)\n",
    "    current_tx_test = standardize_predef(current_tx_test, mean_train, std_train)\n",
    "    '''\n",
    "    # Build poly\n",
    "    current_tx_poly_train = build_poly_tx(current_tx_train, degrees[i])\n",
    "    current_tx_poly_test = build_poly_tx(current_tx_test, degrees[i])\n",
    "    \n",
    "    # Compute best method\n",
    "    current_w, current_loss = ridge_regression(current_y_train, current_tx_poly_train, lambdas[i])\n",
    "    \n",
    "    acc = accuracy(current_y_train, current_tx_poly_train.T, current_w, LOWER_BOUND, UPPER_BOUND)\n",
    "    \n",
    "    print(\"Accuracy:\", acc)\n",
    "    \n",
    "    # Predict\n",
    "    y_test_pred = predict_labels_kaggle(current_w, current_tx_poly_test.T, LOWER_BOUND, UPPER_BOUND)\n",
    "    y_pred[masks_jet_test[i]] = y_test_pred.flatten()\n",
    "\n",
    "print(\"Number of %d:\" % UPPER_BOUND, np.count_nonzero(y_pred == UPPER_BOUND))\n",
    "print(\"Number of %d:\" % LOWER_BOUND, np.count_nonzero(y_pred == LOWER_BOUND))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred, \"test25.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test n : algorithm / features / y / w\n",
    "- - - - - - - - - - - - - - - - - - - \n",
    "Test 1 : least_squares / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 2 : least_squares / corr > 0.1 features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 3 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w\n",
    "\n",
    "Test 4 : least_squares_GD(10000,0.5) / all features standardized / y = -1,1 / random init_w / poly, degree=1\n",
    "\n",
    "Test 5 : least_squares / all features standardized / y = -1,1 / random init_w / median + categorical\n",
    "\n",
    "Test 6 : logistic_regressoin  /all features standardized / y = 0,1 / random init_w / median + categorical + balanced\n",
    "\n",
    "Test 7 : least_squares / all features standardized / y = 0,1 / random init_w\n",
    "\n",
    "Test 8 : Test 1\n",
    "\n",
    "Test 9 : Test 1\n",
    "\n",
    "Test 10 : Test 1 / standardized test_set with mean and std from train_set\n",
    "\n",
    "Test 11 : Test 1 / standardized test_set with mean and std from train_set / balance\n",
    "\n",
    "Test 12 : Ridge regression / non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 13 : Ridge regression / balanced / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 14 : Ridge regression / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11\n",
    "\n",
    "Test 15 : Ridge regression / Removed all rows containing at least a NaN / balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / mean of 4 best lambdas for degree 11 / Replaced NaN values in test_set by median in test_set\n",
    "\n",
    "Test 16 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Ensembling with: (\"least_squares_GD\", 1, 150, 0, 0.01, 0), (\"least_squares_GD\", 1, 50, 0, 0.25, 0), (\"least_squares_SGD\", 1, 30, 256, 0.2, 0), (\"least_squares_SGD\", 1, 60, 64, 0.1, 0), (\"ridge_regression\", 7, 0, 0, 0, 0.001), (\"ridge_regression\", 9, 0, 0, 0, 0.001), (\"ridge_regression\", 11, 0, 0, 0, 0.001)\n",
    "\n",
    "Test 17 : balanced before doing anything / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 11, 11, 11] and lambdas = [0.00046415888336127773, 0.001291549665014884, 4.6415888336127818e-05, 1.0000000000000001e-05]\n",
    "\n",
    "Test 18 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [9, 12, 12, 11] and lambdas = [4.6415888336127818e-05, 0.001, 0.0021544346900318821, 2.1544346900318823e-05]\n",
    "\n",
    "Test 19 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 12, 12, 11] and lambdas = [2.1544346900318823e-05, 0.0021544346900318821, 0.00046415888336127773, 0.0001]\n",
    "\n",
    "Test 20 : balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [8, 13, 12, 10] and lambdas = [0.001, 0.001, 0.0021544346900318821, 0.001]\n",
    "\n",
    "Test 21 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [8, 12, 12] and lambdas = [2.1544346900318823e-05, 0.0021544346900318821, 0.00046415888336127773]\n",
    "\n",
    "Test 22 : non-balanced / standardized test_set with mean and std from train_set BUT AFTER SEPARATING / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [9, 12, 10] and lambdas = [4.6415888336127818e-05, 0.0021544346900318821, 0.00046415888336127773] / New features being inverse log\n",
    "\n",
    "Test 23 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 3 models based on feature 22, using Ridge everytime with degrees = [10, 12, 10] and lambdas = [0.00021544346900318823, 0.00046415888336127773, 0.001] / New features being inverse log\n",
    "\n",
    "Test 24 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [10, 12, 9, 10] and lambdas = [0.00021544346900318823, 0.00046415888336127773, 1.0000000000000001e-05, 0.001] / New features being inverse log\n",
    "\n",
    "Test 25 : non-balanced / standardized test_set with mean and std from train_set / y=-1,1 / Separating into 4 models based on feature 22, using Ridge everytime with degrees = [11, 11, 10, 9] and lambdas = [0.0001, 3.1622776601683795e-05, 0.00031622776601683794, 0.01] / New features being inverse log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- balance output (batch numpy)\n",
    "- median and category\n",
    "- features engineering : features d'intraction\n",
    "- logistic regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
